{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeatureEgineering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "EtRc9_P2CkUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!curl https://sdk.cloud.google.com | bash\n",
        "!gcloud init"
      ],
      "metadata": {
        "id": "h77nSCTEQFrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_name = \"Aladag_sample.csv\"\n",
        "aladag_preprocessed = \"Aladag_sample_preprocessed.csv\"\n",
        "aladag_preprocessed_test = \"Aladag_labeled_preprocessed.csv\"\n",
        "mental_condition_name = \"SMHD_train.csv\"\n",
        "mental_condition_name_test = \"SMHD_test.csv\"\n",
        "\n",
        "mental_condition_name_test = \"SMHD_dev.csv\"\n",
        "model_name = \"\"\n",
        "df_mental_preprocessed = \"df_mental_balanced_preprocessed_preprocessed.csv\"\n",
        "df_mental_preprocessed_test = \"df_mental_valid_preprocessed_preprocessed.csv\""
      ],
      "metadata": {
        "id": "hjzNy8Vckd61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp  gs://masterthesisbert/{df_name} /content/{df_name}\n",
        "!gsutil cp  gs://masterthesisbert/{aladag_preprocessed} /content/{aladag_preprocessed}\n",
        "!gsutil cp  gs://masterthesisbert/{aladag_preprocessed_valid} /content/{aladag_preprocessed_valid}\n",
        "!gsutil cp  gs://masterthesisbert/{mental_condition_name} /content/{mental_condition_name}\n",
        "!gsutil cp  gs://masterthesisbert/{mental_condition_name_test} /content/{mental_condition_name_test}\n",
        "!gsutil cp  gs://masterthesisbert/{mental_condition_name_valid} /content/{mental_condition_name_valid}\n",
        "!gsutil cp  gs://masterthesisbert/{df_mental_preprocessed} /content/{df_mental_preprocessed}\n",
        "!gsutil cp  gs://masterthesisbert/{df_mental_preprocessed_valid} /content/{df_mental_preprocessed_valid}"
      ],
      "metadata": {
        "id": "-jiiAFxzRDEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mental health condition extraction"
      ],
      "metadata": {
        "id": "FyV5tN5j6AI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#only one dataset at the time\n",
        "before_training = False\n",
        "aladag = True\n",
        "preprocess = False\n",
        "reddit_500 = False\n",
        "smhd = False\n",
        "preprocess = False\n",
        "smhd_preprocessed = False\n",
        "\n",
        "if aladag == True:\n",
        "    df_mental = pd.read_csv(aladag_preprocessed)\n",
        "#df_mental = df_mental.astype({\"text\": str, \"label\":str}, errors='raise')\n",
        "    df_mental = df_mental.rename(columns={\"binary_annotation\": \"label\", \"selftext\": \"text\"})\n",
        "\n",
        "    df_mental_test = pd.read_csv(aladag_preprocessed_test)\n",
        "    df_mental_test = df_mental_test.rename(columns={\"binary_annotation\": \"label\", \"selftext\": \"text\"})\n",
        "\n",
        "elif reddit_500:\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_val.csv  /content/reddit_500_final_val.csv\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_train.csv  /content/reddit_500_final_train.csv\n",
        "\n",
        "    df_name2 = \"/content/reddit_500_final_val.csv\"\n",
        "    df_mental_test = pd.read_csv(df_name2)\n",
        "    \n",
        "\n",
        "    df_name = \"/content/reddit_500_final_train.csv\"\n",
        "    df_mental= pd.read_csv(df_name)\n",
        "\n",
        "\n",
        "    df_mental = df_mental.rename(columns={\"Label\": \"label\", \"selftext\": \"text\"})\n",
        "    df_mental_test = df_mental_test.rename(columns={\"Label\": \"label\", \"selftext\": \"text\"})\n",
        "\n",
        "\n",
        "elif (smhd == True and preprocess == False):\n",
        "\n",
        "    #add the names of finaly preprocessed dataframes here \n",
        "    !gsutil cp  gs://masterthesisbert/df_mental_balanced_preprocessed_preprocessed.csv /content/df_mental_balanced_preprocessed_preprocessed.csv\n",
        "    !gsutil cp  gs://masterthesisbert/df_mental_valid_preprocessed_preprocessed.csv /content/df_mental_valid_preprocessed_preprocessed.csv\n",
        "\n",
        "!gsutil cp  gs://masterthesisbert/finetuned_BERT_10classes_epoch_3.model /content/finetuned_BERT_10classes_epoch_3.model"
      ],
      "metadata": {
        "id": "1gvc5Cd36Rok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "avKY9F52SW8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "n8xtcmcMSsjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
        "                                          do_lower_case=True)                                          "
      ],
      "metadata": {
        "id": "wbzd9rrUuXwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    df_mental.text.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=256, \n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "encoded_data_val= tokenizer.batch_encode_plus(\n",
        "    df_mental_test.text.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=256, \n",
        "    return_tensors='pt'\n",
        ")\n"
      ],
      "metadata": {
        "id": "1tGb-2TaS98q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "possible_labels = sorted(df_mental.label.unique())\n",
        "\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index\n",
        "label_dict"
      ],
      "metadata": {
        "id": "PPXTw3Ohlvwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental['code'] = df_mental.label.replace(label_dict)\n",
        "df_mental_test['code'] = df_mental_test.label.replace(label_dict)"
      ],
      "metadata": {
        "id": "uICUNPleo6_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_test.code.value_counts()\n",
        "len(df_mental.code.value_counts())"
      ],
      "metadata": {
        "id": "NJxoV3oBfIXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental.label.value_counts()"
      ],
      "metadata": {
        "id": "WZWACrsAYXUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(df_mental.code.values)\n",
        "\n",
        "input_ids_test = encoded_data_test['input_ids']\n",
        "attention_masks_test = encoded_data_test['attention_mask']\n",
        "labels_test = torch.tensor(df_mental_test.code.values)\n",
        "\n",
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
        "dataset_val = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
      ],
      "metadata": {
        "id": "BOX8SWsRS9-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 3\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, \n",
        "                              sampler=RandomSampler(df_mental), \n",
        "                              batch_size=batch_size)\n",
        "\n",
        "dataloader_test = DataLoader(dataset_test, \n",
        "                                   sampler=SequentialSampler(df_mental_test), \n",
        "                                   batch_size=batch_size)"
      ],
      "metadata": {
        "id": "SlbBXHvXfnki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=10,\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)"
      ],
      "metadata": {
        "id": "Ymdm8xkNG0Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=1e-5, \n",
        "                  eps=1e-8)\n",
        "                  \n",
        "epochs = 3\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(dataloader_train)*epochs)\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "def accuracy_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return accuracy_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "def precision_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return precision_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "def recall_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return recall_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "\n",
        "def accuracy_per_class(preds, labels):\n",
        "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
        "    \n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    for label in np.unique(labels_flat):\n",
        "        y_preds = preds_flat[labels_flat==label]\n",
        "        y_true = labels_flat[labels_flat==label]\n",
        "        print(f'Class: {label_dict_inverse[label]}')\n",
        "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "dkkJh1M3gO86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "def evaluate(dataloader_val, only_predict = False):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "    \n",
        "    \n",
        "        \n",
        "    if only_predict:\n",
        "\n",
        "        for batch in dataloader_val:\n",
        "        \n",
        "            batch = tuple(b.to(device) for b in batch)\n",
        "            \n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1]\n",
        "                    }\n",
        "       \n",
        "            with torch.no_grad():        \n",
        "                outputs = model(**inputs)\n",
        "         \n",
        "            logits = outputs[0]\n",
        "            \n",
        "            logits = logits.detach().cpu().numpy()\n",
        "      \n",
        "            predictions.append(logits)\n",
        "            \n",
        "\n",
        "        predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "        for batch in dataloader_val:\n",
        "        \n",
        "            batch = tuple(b.to(device) for b in batch)\n",
        "            \n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'labels':         batch[2],\n",
        "                    }\n",
        "\n",
        "            with torch.no_grad():        \n",
        "                outputs = model(**inputs)\n",
        "\n",
        "\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "            loss_val_total += loss.item()\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = inputs['labels'].cpu().numpy()\n",
        "            predictions.append(logits)\n",
        "            true_vals.append(label_ids)\n",
        "    \n",
        "        loss_val_avg = loss_val_total/len(dataloader_val) \n",
        "        \n",
        "        predictions = np.concatenate(predictions, axis=0)\n",
        "        true_vals = np.concatenate(true_vals, axis=0)\n",
        "                \n",
        "        return loss_val_avg, predictions, true_vals"
      ],
      "metadata": {
        "id": "IluwSE1TGfeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('finetuned_BERT_10classes_epoch_3.model', map_location=torch.device('cpu')))\n",
        "\n",
        "#loss_val_avg, predictions, true_vals = evaluate(dataloader_validation)"
      ],
      "metadata": {
        "id": "vXzarnRg6iW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_train = evaluate(dataloader_train, only_predict = True)"
      ],
      "metadata": {
        "id": "FLO4CgsVNUYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = evaluate(dataloader_test, only_predict = True)"
      ],
      "metadata": {
        "id": "gAu7WItagyFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [\"anxiety\", \"depression\", \"control\", \"ADHD\", \"bipolar disorder\", \"autism\", \"PTSD\", \"OCD\", \"schizophrenia\", \"eating disorder\"]\n",
        "diseases = sorted(map(lambda x: x.lower(), a))"
      ],
      "metadata": {
        "id": "UBJ8HhPx1vJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mental_results_test = pd.DataFrame(predictions_test, columns=diseases)"
      ],
      "metadata": {
        "id": "DbXUUhV_zP1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mental_results_train = pd.DataFrame(predictions_train, columns=diseases)"
      ],
      "metadata": {
        "id": "5pFM0gbLg4Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_test = pd.concat([df_mental_test, mental_results_test], axis=1)"
      ],
      "metadata": {
        "id": "XzLsd-Ko6ZP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_train = pd.concat([df_mental, mental_results_train], axis=1)"
      ],
      "metadata": {
        "id": "2hQY8xC8g6BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract emotions"
      ],
      "metadata": {
        "id": "D77HnMyg6UA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4 basic emotions from text"
      ],
      "metadata": {
        "id": "AIqB4k1BVGXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "trSCm3pzqFRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "RnBkbwu9qe_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MilaNLProc/xlm-emo.git"
      ],
      "metadata": {
        "id": "mPWub06MqsOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2eae0a-aa2d-46ba-920f-affd2ff6d5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'xlm-emo'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 90 (delta 40), reused 69 (delta 24), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (90/90), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/xlm-emo /content/xlm_emo"
      ],
      "metadata": {
        "id": "Hfx3vR0OB1Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from xlm_emo.xlm_emo.dataset import prepare_dataset\n",
        "\n",
        "class EmotionClassifier:\n",
        "\n",
        "        def __init__(self, model=\"t\"):\n",
        "            if model == \"t\":\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(\"MilaNLProc/xlm-emo-t\")\n",
        "                self.model = AutoModelForSequenceClassification.from_pretrained(\"MilaNLProc/xlm-emo-t\")\n",
        "            else:\n",
        "                raise Exception(\"Not Yet Implemented\")\n",
        "\n",
        "        def predict(self, text: List):\n",
        "\n",
        "            df = pd.DataFrame({\"texts\": text})\n",
        "\n",
        "            train_dataset = Dataset.from_pandas(df)\n",
        "            train_dataset = prepare_dataset(train_dataset, self.tokenizer)\n",
        "\n",
        "            trainer = Trainer(model=self.model)\n",
        "\n",
        "            local_results = trainer.predict(train_dataset)\n",
        "\n",
        "            mapper = {0: \"anger\", 1: \"fear\", 2: \"joy\", 3: \"sadness\"}\n",
        "\n",
        "            return local_results"
      ],
      "metadata": {
        "id": "DHmrJJ6nqBHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ec = EmotionClassifier()"
      ],
      "metadata": {
        "id": "tgwG37aErSlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emot_test = ec.predict(df_mental_test[\"text\"].tolist())[0]"
      ],
      "metadata": {
        "id": "siolyGBCsVf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emot_train = ec.predict(df_mental[\"text\"].tolist())[0]"
      ],
      "metadata": {
        "id": "b2OHziw5e3sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_columns = [\"anger\", \"fear\", \"joy\", \"sadness\"]"
      ],
      "metadata": {
        "id": "1_FfJaOFiY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emot_test_df = pd.DataFrame(emot_test, columns=emotion_columns)"
      ],
      "metadata": {
        "id": "8_z6_ARWfBbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emot_train_df = pd.DataFrame(emot_train, columns=emotion_columns)"
      ],
      "metadata": {
        "id": "nF5s_P2wnDn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_test = pd.concat([df_mental_test, emot_test_df], axis=1)"
      ],
      "metadata": {
        "id": "eaqCRvK6n6_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_train = pd.concat([df_mental_train, emot_train_df], axis=1)"
      ],
      "metadata": {
        "id": "9iuxFGIJoBVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_train"
      ],
      "metadata": {
        "id": "qQ6pqGm9Jj1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract sentiment"
      ],
      "metadata": {
        "id": "tatAPJ5Xlif4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request\n",
        "\n",
        "# Preprocess text (username and link placeholders)\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        " \n",
        " \n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "task='sentiment'\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}-latest\"\n",
        "model_senti = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# download label mapping\n",
        "labels=[]\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "    csvreader = csv.reader(html, delimiter='\\t')\n",
        "labels = [row[1] for row in csvreader if len(row) > 1]"
      ],
      "metadata": {
        "id": "sRIqnw_ffyZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_test[[\"negative\", \"neutral\", \"positive\"]] = 0.0\n",
        "df_mental_train[[\"negative\", \"neutral\", \"positive\"]] = 0.0"
      ],
      "metadata": {
        "id": "s-XyuihKztmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(df):\n",
        "    for i in range(len(df)):\n",
        "        text = preprocess(df.text[i])\n",
        "        encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "        output = model_senti(**encoded_input)\n",
        "      \n",
        "        scores = output[0][0].detach().numpy()\n",
        "        scores = softmax(scores)\n",
        "        \n",
        "        for j in range(3):\n",
        "\n",
        "            df.iloc[[i], [len(df.columns)-3+j]] = scores[j]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ueIHYkq0f2X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_test = get_sentiment(df_mental_test)"
      ],
      "metadata": {
        "id": "xGMQJ_nW-yN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_train = get_sentiment(df_mental_train)"
      ],
      "metadata": {
        "id": "XwRIMATHtbap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3rd and 1st person pronouns ratio "
      ],
      "metadata": {
        "id": "NECLjrRe52WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "third = [\"he \", \"he'\" \"she \", \"she'\", \"it \", \"it'\", \"one \",  \"they \", \"they'\", \"him \", \"her \", \"it \", \"their \",  \" them \", \"his \", \"hers \", \" theirs \", \" himself \", \" herself \", \" itself \", \" oneself \",  \" themselves \"] \n",
        "first = [\" i \", \"i'\", \"we \", \"we'\",\" me \", \" us \", \" myself \", \" mine \", \" ours \", \" myself \", \" ourselves \" ]\n",
        "second = [\"you\", \"you'\", \" u \", \"u'\",  \"yours\", \"yourself\", \"yourselves\" ,\" u \"]"
      ],
      "metadata": {
        "id": "ncPwRH83f89V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_test[[\"first\",\"third\"]] = 0.0"
      ],
      "metadata": {
        "id": "5Q83JFZb9vOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_train[[\"first\",\"third\"]] = 0.0"
      ],
      "metadata": {
        "id": "HTgr-xJ_z8s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ratio(df):\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        count_third = 0\n",
        "        count_first = 0\n",
        "        count_second = 0\n",
        "        for j in third:\n",
        "            count_third +=  df.text[i].lower().count(j)\n",
        "        for k in first:\n",
        "            count_first += df.text[i].lower().count(k)\n",
        "        for k in second:\n",
        "            count_second += df.text[i].lower().count(k)\n",
        "        \n",
        "        words = len(df.text[i])\n",
        "        third_ratio = count_third/words\n",
        "        first_ratio = count_first/words\n",
        "        second_ratio = count_second/words\n",
        "      \n",
        "        #df[\"third\"][i] = third_ratio/(third_ratio + first_ratio + second_ratio)\n",
        "\n",
        "        df[\"third\"][i] = third_ratio/words\n",
        "\n",
        "        df[\"first\"][i] = first_ratio/words\n",
        "\n",
        "    return df   "
      ],
      "metadata": {
        "id": "KoA0FYp49vXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_test = get_ratio(df_mental_test)"
      ],
      "metadata": {
        "id": "lDABXHQ6CP3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_train = get_ratio(df_mental_train)"
      ],
      "metadata": {
        "id": "ie2wVa3t9ve8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mental_train"
      ],
      "metadata": {
        "id": "nWaHiS5JT7m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if aladag:\n",
        "    df_mental_test.to_csv(\"Aladag_labeled_preprocessed_features.csv\")\n",
        "    df_mental_train.to_csv(\"Aladag_sample_preprocessed_features.csv\")\n",
        "    !gsutil cp  /content/Aladag_labeled_preprocessed_features.csv gs://masterthesisbert/Aladag_labeled_preprocessed_features.csv \n",
        "    !gsutil cp   /content/Aladag_sample_preprocessed_features.csv gs://masterthesisbert/Aladag_sample_preprocessed_features.csv\n",
        "\n",
        "elif reddit_500:\n",
        "    df_mental_test.to_csv(\"reddit_500_final_val_features.csv\")\n",
        "    df_mental_train.to_csv(\"reddit_500_final_train_features.csv\")\n",
        "    !gsutil cp  /content/reddit_500_final_val_features.csv gs://masterthesisbert/reddit_500_final_val_features.csv \n",
        "    !gsutil cp   /content/reddit_500_final_train_features.csv gs://masterthesisbert/reddit_500_final_train_features.csv"
      ],
      "metadata": {
        "id": "JwEcmi-b1MOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}