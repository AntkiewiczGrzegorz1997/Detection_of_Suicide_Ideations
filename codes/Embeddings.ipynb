{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBDhxln6tDpA"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install \"tensorflow_hub>=0.6.0\"\n",
        "!pip3 install tensorflow_text==1.15\n",
        "!pip install transformers\n",
        "!pip install contractions\n",
        "!pip install emoji==1.6.3\n",
        "!pip install emot==2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlKRy5-Mw7FH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkMd-XoH4dSc"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!curl https://sdk.cloud.google.com | bash\n",
        "\n",
        "\n",
        "!gcloud init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj0QMYi-0t0x"
      },
      "outputs": [],
      "source": [
        "#!gsutil -m cp gs://masterthesisbert/bert_model_BERT_RE_long/bert-reddit-vocab.txt gs://masterthesisbert/bert_model_BERT_RE_wide/bert-reddit-vocab.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84FexNEhyuRo"
      },
      "outputs": [],
      "source": [
        "my_bert = False\n",
        "my_first_bert=False\n",
        "my_second_bert=False\n",
        "\n",
        "\n",
        "if (my_bert == True):\n",
        "\n",
        "    #Important. Set your parameters\n",
        "    bert_dir_name = \"masterthesisbert\"\n",
        "    !mkdir {bert_dir_name}\n",
        "    path_to_bert_folder = \"masterthesisbert/bert_model_BERT_RE_long\"\n",
        "    path_last_bert_folder = \"bert_model_BERT_RE_long\"\n",
        "\n",
        "    #!gsutil -m cp -r gs://{path_to_bert_folder} /content/{bert_dir_name}\n",
        "\n",
        "    #some parameters should be put here as well\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/{path_last_bert_folder}/bert-reddit-vocab.txt /content/bert-vocab.txt\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/{path_last_bert_folder}/model.ckpt-1000000.data-00000-of-00001 /content/\n",
        "    !gsutil cp gs://masterthesisbert/{path_last_bert_folder}/bert_config.json /content/\n",
        "    !gsutil cp gs://masterthesisbert/{path_last_bert_folder}/model.ckpt-1000000.index /content/\n",
        "    !gsutil cp gs://masterthesisbert/{path_last_bert_folder}/model.ckpt-1000000.meta /content/\n",
        "    \n",
        "    !transformers-cli convert --model_type bert --tf_checkpoint /content/model.ckpt-1000000 --config /content/bert_config.json --pytorch_dump_output /content/pytorch_model.bin\n",
        "\n",
        "    # importing bert-base, tokenizers, and models from libraries\n",
        "    model_class, tokenizer_class, pretrained_weights, pretrained_tokenizer = (ppb.BertModel, ppb.BertTokenizer, '/content/pytorch_model.bin', '/content/bert-vocab.txt')\n",
        "    tokenizer = tokenizer_class.from_pretrained(pretrained_tokenizer)\n",
        "    model = model_class.from_pretrained(pretrained_weights, config=\"bert_config.json\")\n",
        "\n",
        "else:\n",
        "\n",
        "    # importing bert-base, tokenizers, and models from libraries\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "    model = model_class.from_pretrained(pretrained_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM5Qas4LDFnO"
      },
      "outputs": [],
      "source": [
        "aladag = False\n",
        "reddit_500 = False\n",
        "eight_classes=False\n",
        "depression=False\n",
        "labaled_data = False\n",
        "preprocessing_done = True\n",
        "rsdd=True\n",
        "preprocessing_done_final=True\n",
        "balance_smhd = False "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if balance_smhd:\n",
        "    mental_condition_name = \"SMHD_train.csv\"\n",
        "    mental_condition_name_test = \"SMHD_test.csv\"\n",
        "    mental_condition_name_valid = \"SMHD_dev.csv\"\n",
        "\n",
        "    !gsutil cp  gs://masterthesisbert/{mental_condition_name} /content/{mental_condition_name}\n",
        "    !gsutil cp  gs://masterthesisbert/{mental_condition_name_test} /content/{mental_condition_name_test}\n",
        "    !gsutil cp  gs://masterthesisbert/{mental_condition_name_valid} /content/{mental_condition_name_valid}\n",
        "\n",
        "    df_mental = pd.read_csv(mental_condition_name)\n",
        "    df_mental_valid = pd.read_csv(mental_condition_name_test)\n",
        "    df_mental_test = pd.read_csv(mental_condition_name_valid)\n",
        "    df_mental = pd.concat([df_mental, df_mental_valid], axis=0).reset_index()\n",
        "\n",
        "    df_mental_control2 = df_mental[df_mental['label'] == 'control'].copy().sample(frac=.98)\n",
        "    df_mental_test_control2 = df_mental_test[df_mental_test['label'] == 'control'].copy().sample(frac=.98)\n",
        "    df_mental_test.drop(df_mental_test_control2.index, inplace=True)\n",
        "    df_mental_test.reset_index(inplace=True)\n",
        "    df_mental.drop(df_mental_control2.index, inplace=True)\n",
        "    df_mental.reset_index(inplace=True)\n",
        "    a = [\"control\", \"adhd\", \"anxiety\", \"depression\",\"bipolar\", \"autism\", \"ptsd\",\"ocd\", \"eating\", \"schizophrenia\"]\n",
        "    df_mental = df_mental[df_mental['label'].isin(a)]\n",
        "    df_mental_test = df_mental_test[df_mental_test['label'].isin(a)]\n",
        "    b = [\"[removed]\", \".\", \"Added\", \"No\", \"Nice\"]\n",
        "    df_mental = df_mental[~df_mental['text'].isin(b)]\n",
        "    df_mental_test = df_mental_test[~df_mental_test['text'].isin(b)]\n",
        "    df_mental = df_mental[~df_mental['text'].isna()]\n",
        "    df_mental_test = df_mental_test[~df_mental_test['text'].isna()]\n",
        "\n",
        "    import re\n",
        "    import contractions \n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    def clean_text_small(text):\n",
        "\n",
        "        text = re.sub(r'\\n', '', text)\n",
        "        ## Remove html content\n",
        "        text = BeautifulSoup(text, \"html.parser\").text\n",
        "\n",
        "        ## Remove contractions\n",
        "        text = contractions.fix(text)\n",
        "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "        text = text.lower()\n",
        "        return text\n",
        "\n",
        "    df_mental[\"text\"] = df_mental.apply(lambda x: clean_text_small(x[\"text\"]), axis=1)\n",
        "    df_mental_test[\"text\"] = df_mental_test.apply(lambda x: clean_text_small(x[\"text\"]), axis=1)\n",
        "    df_mental_test\n",
        "\n",
        "    df_mental[\"text\"].dropna(axis =0, inplace=True)\n",
        "    df_mental_test[\"text\"].dropna(axis =0, inplace=True)\n",
        "\n",
        "    df_mental = df_mental[df_mental['text'].str.split().str.len().gt(10)]\n",
        "\n",
        "    df_mental_test = df_mental_test[df_mental_test['text'].str.split().str.len().gt(10)]\n",
        "\n",
        "    sample_amounts = {'adhd': 1500,\n",
        "                      'anxiety': 1500,\n",
        "                      'autism': 900,\n",
        "                      'bipolar': 1000,\n",
        "                      'control': 1500,\n",
        "                      'depression': 1500,\n",
        "                      'eating': 350,\n",
        "                      'ocd': 500,\n",
        "                      'ptsd': 800,\n",
        "                      'schizophrenia': 450}\n",
        "\n",
        "\n",
        "    df_mental = (\n",
        "        df_mental.groupby('label').apply(lambda g: g.sample(\n",
        "            # lookup number of samples to take\n",
        "            n=sample_amounts[g.name],\n",
        "            # enable replacement if len is less than number of samples expected\n",
        "            replace=len(g) < sample_amounts[g.name]  \n",
        "        )).droplevel(0)\n",
        "    )\n",
        "\n",
        "    n=2000\n",
        "    df_mental_test = df_mental_valid.sample(n, replace=False)\n",
        "    df_mental.drop(columns=[\"level_0\"], inplace = True)\n",
        "    df_mental = df_mental.sample(frac=1).reset_index(drop=True)\n",
        "    df_mental_test = df_mental_valid.sample(frac=1).reset_index(drop=True)\n",
        "    df_mental.to_csv(\"df_mental_balanced_preprocessed.csv\")\n",
        "    df_mental_test.to_csv(\"df_mental_valid_preprocessed.csv\")\n",
        "\n",
        "    # Is commented out so that I dont overwrite my code \n",
        "    #!gsutil cp  /content/df_mental_balanced_preprocessed.csv gs://masterthesisbert/df_mental_balanced_preprocessed.csv\n",
        "    #!gsutil cp  /content/df_mental_valid_preprocessed.csv gs://masterthesisbert/df_mental_valid_preprocessed.csv"
      ],
      "metadata": {
        "id": "jIpxzHHhxPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uge-xRZA3NY7"
      },
      "outputs": [],
      "source": [
        "if preprocessing_done:\n",
        "    !gsutil cp gs://masterthesisbert/df_mental_balanced_preprocessed.csv /content/df_mental_balanced_preprocessed.csv\n",
        "    !gsutil cp gs://masterthesisbert/df_mental_valid_preprocessed.csv /content/df_mental_valid_preprocessed.csv\n",
        "    !gsutil cp gs://masterthesisbert/df_mental_balanced_preprocessed_preprocessed.csv /content/df_mental_balanced_preprocessed_preprocessed.csv\n",
        "    !gsutil cp gs://masterthesisbert/df_mental_valid_preprocessed_preprocessed.csv /content/df_mental_valid_preprocessed_preprocessed.csv\n",
        "    !gsutil cp gs://masterthesisbert/rsdd_train_preprocessed.csv /content/rsdd_train_preprocessed.csv\n",
        "    !gsutil cp gs://masterthesisbert/rsdd_valid_preprocessed.csv /content/rsdd_valid_preprocessed.csv\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_train.csv /content/reddit_500_final_train.csv \n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_val.csv /content/reddit_500_final_val.csv\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_sample_preprocessed.csv /content/Aladag_sample_preprocessed.csv\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_labeled_preprocessed.csv /content/Aladag_labeled_preprocessed.csv\n",
        "    \n",
        "\n",
        "\n",
        "else:\n",
        "\n",
        "\n",
        "    if aladag:\n",
        "        !gsutil cp gs://masterthesisbert/Aladag_sample.csv /content/Aladag_sample.csv\n",
        "        !gsutil cp gs://masterthesisbert/SuicideAladag2018_preprocessed.csv /content\n",
        "\n",
        "        \n",
        "\n",
        "    elif eight_classes==True:\n",
        "        !gsutil cp gs://masterthesisbert/df_mental_balanced.csv  /content/df_mental_balanced.csv\n",
        "        !gsutil cp gs://masterthesisbert/df_mental_valid_preprocessed.csv /content/df_mental_valid_preprocessed.csv\n",
        "\n",
        "    elif reddit_500:\n",
        "        !gsutil cp gs://masterthesisbert/500_Reddit_users_posts_labels_preprocessed.csv /content/500_Reddit_users_posts_labels_preprocessed.csv\n",
        "\n",
        "    elif rsdd: \n",
        "        !gsutil cp gs://masterthesisbert/RSDD_train.csv /content/RSDD_train.csv\n",
        "        !gsutil cp gs://masterthesisbert/RSDD_valid.csv /content/RSDD_valid.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI66MZ_pWpi4"
      },
      "outputs": [],
      "source": [
        "# Look at the below comments to determine whether you want to output the 2-dimensional or 3-dimensional BERT features.\n",
        "def getFeatures(batch_1, tokenizer, model):\n",
        "\n",
        "  tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, padding='max_length', truncation=True, max_length=256)))\n",
        "\n",
        "  max_len = 0\n",
        "  for i in tokenized.values:\n",
        "      if len(i) > max_len:\n",
        "          max_len = len(i)\n",
        "\n",
        "  padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "\n",
        "  attention_mask = np.where(padded != 0, 1, 0)\n",
        "  attention_mask.shape\n",
        "\n",
        "\n",
        "  input_ids = torch.tensor(padded)  \n",
        "  attention_mask = torch.tensor(attention_mask)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "  \"\"\"\n",
        "  hidden_states = last_hidden_states[2]\n",
        "\n",
        "  check_1 = torch.add(hidden_states[12], hidden_states[11])\n",
        "  check_2 = torch.add(hidden_states[10], hidden_states[9])\n",
        "  check_3 = torch.add(check_1, check_2)\n",
        "\n",
        "  #features = check_3.numpy()\n",
        "  features = check_3[:,0,:].numpy()\n",
        "  \"\"\"\n",
        "  \n",
        "  features_2D = last_hidden_states[0][:,0,:].numpy() # use this line if you want the 2D BERT features\n",
        "  features_3D = last_hidden_states[0].numpy() # use this line if you want the 3D BERT features \n",
        "\n",
        "  return features_2D, features_3D "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0QdIA8rXigM"
      },
      "outputs": [],
      "source": [
        "bucket_name = 'masterthesisbert'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axjNChJ90Cow"
      },
      "outputs": [],
      "source": [
        "#when using google drive\n",
        "drive = False\n",
        "if drive==True:\n",
        "    addition = \"drive/MyDrive/\"\n",
        "else:\n",
        "    addition = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL1l2MxH0Tf3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "if(preprocessing_done==False):\n",
        "    if aladag == True:\n",
        "      \n",
        "        df_name = \"SuicideAladag2018_preprocessed.csv\"\n",
        "\n",
        "        \n",
        "    elif reddit_500 == True:\n",
        "\n",
        "        df_name = \"500_Reddit_users_posts_labels_preprocessed.csv\"\n",
        "\n",
        "    elif eight_classes:\n",
        "\n",
        "        if labaled_data:\n",
        "\n",
        "            df_name = \"df_mental_valid_preprocessed.csv\"\n",
        "\n",
        "        else: \n",
        "\n",
        "            df_name = \"df_mental_balanced_preprocessed.csv\"\n",
        "\n",
        "    elif rsdd:\n",
        "\n",
        "        if labaled_data:\n",
        "\n",
        "            df_name = \"RSDD_train.csv\"\n",
        "\n",
        "        else: \n",
        "\n",
        "            df_name = \"RSDD_valid.csv\"\n",
        "\n",
        "    df = pd.read_csv(df_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoRnvFmqXOtu"
      },
      "outputs": [],
      "source": [
        "if eight_classes and preprocessing_done==False:\n",
        "    output = pd.read_csv(df_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if (preprocessing_done==False and aladag==True):\n",
        "    # co zrobic z tym koda \n",
        "    df_annotated = output[~np.isnan(output[\"binary_annotation\"])].copy()\n",
        "    df_annotated = df_annotated[(df_annotated[\"subreddit\"] == 'SuicideWatch') | (df_annotated[\"subreddit\"] == 'Showerthoughts')]\n",
        "    df_annotated.to_csv(\"labeled_Aladag.csv\")\n",
        "    df_annotated.subreddit.value_counts()\n",
        "    !gsutil cp /content/labeled_Aladag.csv gs://{bucket_name}/labeled_Aladag.csv\n",
        "    print(len(df))\n",
        "    df = df[~df[\"Unnamed: 0\"].isin(df_annotated[\"Unnamed: 0\"])].copy()\n",
        "    #We have to perform now everything for two datasets \n",
        "\n",
        "    print(len(df))\n",
        "\n",
        "    df = df[(df.subreddit == \"SuicideWatch\") | (df.subreddit == \"Showerthoughts\")].copy()\n",
        "    print(len(df))\n",
        "\n",
        "    samples_per_group_dict = {'SuicideWatch': 5000, \n",
        "                              'Showerthoughts': 5000}\n",
        "                              \n",
        "    output = df.groupby('subreddit').apply(lambda group: group.sample(samples_per_group_dict[group.name])).reset_index(drop = True)\n",
        "\n",
        "    output = output.sample(frac = 1)\n",
        "    output.subreddit.value_counts()\n",
        "    # for training purposes we have to assume that all post from SuicideWatch in df are 1 and all posts from Showerthoughts are 0 \n",
        "    output[\"binary_annotation\"] = output.apply(lambda x: 1 if (x[\"subreddit\"] == \"SuicideWatch\") else 0, axis=1)\n",
        "    output.reset_index(inplace=True)\n",
        "    output.to_csv(\"Aladag_sample.csv\")\n",
        "    output = pd.read_csv(\"Aladag_sample.csv\")\n",
        "    output"
      ],
      "metadata": {
        "id": "XzRcgQbB94kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HllTzkdXhRr"
      },
      "source": [
        "#Clean Text Aladag so that we can create embeddings on cleaned text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name=\"masterthesisbert\""
      ],
      "metadata": {
        "id": "j-VHAsvXiAkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eayBeiYuv6lb",
        "outputId": "cc30907a-7445-41f1-a3b0-d4f08e58af98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://masterthesisbert/Emoji_Dict.p...\n",
            "/ [0 files][    0.0 B/ 88.1 KiB]                                                \r/ [1 files][ 88.1 KiB/ 88.1 KiB]                                                \r\n",
            "Operation completed over 1 objects/88.1 KiB.                                     \n",
            "Copying gs://masterthesisbert/Emoticon_Dict.p...\n",
            "/ [1 files][  3.4 KiB/  3.4 KiB]                                                \n",
            "Operation completed over 1 objects/3.4 KiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://{bucket_name}/Emoji_Dict.p /content/Emoji_Dict.p \n",
        "!gsutil cp gs://{bucket_name}/Emoticon_Dict.p /content/Emoticon_Dict.p "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ktM-fwdkZG"
      },
      "outputs": [],
      "source": [
        "from emot.emo_unicode import EMOTICONS\n",
        "import pickle\n",
        "import emoji\n",
        "with open('Emoji_Dict.p', 'rb') as fp:\n",
        "    Emoji_Dict = pickle.load(fp)\n",
        "\n",
        "with open('Emoticon_Dict.p', 'rb') as fp:\n",
        "    Emoticon_Dict = pickle.load(fp)\n",
        "\n",
        "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB8FG9zoZJn9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def convert_emoticons_to_word(text):\n",
        "    for emot in EMOTICONS:\n",
        "\n",
        "        text = re.sub(u'(' + emot + ')', \"_\".join(EMOTICONS[emot].replace(\",\", \"\").split()), text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_emojis_to_word(text):\n",
        "\n",
        "    text = emoji.demojize(text, delimiters=(\"\", \"\"))\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "\n",
        "    text = re.sub(r\"\\-\", \" \", text)\n",
        "\n",
        "    text = re.sub(r\"\\=\", \" \", text)\n",
        "    text = re.sub(r\"\\,\", \" , \", text)\n",
        "    text = re.sub(r\"\\.\", \" . \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\<\", \" \", text)\n",
        "    text = re.sub(r\"\\;\", \" ; \", text)\n",
        "    text = re.sub(r\"\\=\", \" \", text)\n",
        "    text = re.sub(r\",\", \" , \", text)\n",
        "\n",
        "\n",
        "    # text = re.sub(r\"\\'s\", \" \", text)\n",
        "\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\" ve \", \" have \", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" d \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"y'\", \"you \", text)\n",
        "\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" usa \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \" 911 \", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "\n",
        "    #text = re.sub(r\"[^A-Za-z0-9^,!.]\", \" \", text)\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "\n",
        "    ## Remove html content\n",
        "    text = BeautifulSoup(text, \"html.parser\").text\n",
        "\n",
        "    ## Remove contractions\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ULKeDExK7P"
      },
      "source": [
        "#Labeled data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data=False"
      ],
      "metadata": {
        "id": "vtYtHRFqiQOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAggLaaFed33"
      },
      "outputs": [],
      "source": [
        "if(preprocessing_done==False):\n",
        "    if labeled_data and aladag:\n",
        "\n",
        "      output = df_annotated.copy()\n",
        "      \n",
        "    else:\n",
        "      output = df.copy()\n",
        "\n",
        "if eight_classes:\n",
        "    column = \"text\"\n",
        "elif rsdd:\n",
        "    column = \"posts\"\n",
        "\n",
        "\n",
        "else: column = \"selftext\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCSvO6eQS3pB"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(output):\n",
        "    output[column] = output.apply(lambda x: convert_emojis_to_word(str(x[column])), axis=1)\n",
        "\n",
        "    output[column] = output.apply(lambda x: convert_emoticons_to_word(x[column]), axis=1)\n",
        "\n",
        "    output[column] = output.apply(lambda x: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x[column], flags=re.MULTILINE), axis=1)\n",
        "\n",
        "    output[column] = output.apply(lambda x: x[column].lower(), axis=1)\n",
        "\n",
        "    output[column] = output.apply(lambda x: clean_text(x[column]), axis=1)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZcI7p9sfuPF"
      },
      "outputs": [],
      "source": [
        "# supportive is not ideation, everything else is\n",
        "\n",
        "if (reddit_500==True and preprocessing_done == False):\n",
        "\n",
        "    df[\"Label\"] = df.apply(lambda x: 1 if x[\"Label\"] in [\"Ideation\", \"Attempt\", \"Behavior\"] else 0, axis=1)\n",
        "    \n",
        "    df = preprocess_text(df)\n",
        "\n",
        "    # for test we do it like that \n",
        "    df1 = df.query('Label == 0').sample(3250)\n",
        "    df2 = df.query('Label == 1').sample(3250)\n",
        "    df = pd.concat([df1, df2])\n",
        "    dftest1 = df.query('Label == 0').sample(1000)\n",
        "    dftest2 = df.query('Label == 1').sample(1000)\n",
        "    dftest = pd.concat([dftest1, dftest2])\n",
        "    df = pd.merge(df,dftest, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
        "    dftest.reset_index(inplace=True)\n",
        "    df.reset_index(inplace=True)\n",
        "    df.to_csv(\"reddit_500_final_train.csv\")\n",
        "    dftest.to_csv(\"reddit_500_final_val.csv\")\n",
        "    #if we wanna load it to a disc\n",
        "    #!gsutil cp /content/reddit_500_final_train.csv gs://masterthesisbert/reddit_500_final_train.csv\n",
        "    #!gsutil cp /content/reddit_500_final_val.csv gs://masterthesisbert/reddit_500_final_val.csv\n",
        "\n",
        "\n",
        "elif(rsdd == True and preprocessing_done ==False):\n",
        "\n",
        "    output = output[output['posts'].str.split().str.len().gt(10)]\n",
        "\n",
        "    output = preprocess_text(output)\n",
        "\n",
        "    output[\"label\"] = output.apply(lambda x: 1 if x[\"label\"] in [\"depression\"] else 0, axis=1)\n",
        "\n",
        "\n",
        "    if labaled_data: \n",
        "        df1 = output.query('label == 0').sample(1000)\n",
        "        df2 = output.query('label == 1').sample(1000)\n",
        "        output = pd.concat([df1, df2])\n",
        "\n",
        "        output = output.sample(frac=1)\n",
        "\n",
        "        output.to_csv(\"rsdd_valid_preprocessed.csv\")\n",
        "\n",
        "        !gsutil cp /content/rsdd_valid_preprocessed.csv gs://masterthesisbert/rsdd_valid_preprocessed.csv\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "        df1 = output.query('label == 0').sample(2500)\n",
        "        df2 = output.query('label == 1').sample(2500)\n",
        "        output = pd.concat([df1, df2])\n",
        "\n",
        "        output = output.sample(frac=1)\n",
        "\n",
        "        output.to_csv(\"rsdd_train_preprocessed.csv\")\n",
        "\n",
        "        !gsutil cp /content/rsdd_train_preprocessed.csv gs://masterthesisbert/rsdd_train_preprocessed.csv\n",
        "\n",
        "elif(aladag == True and preprocessing_done == False):\n",
        "    output = preprocess_text(output)\n",
        "\n",
        "    if labaled_data:\n",
        "\n",
        "        output.to_csv(\"Aladag_labeled_preprocessed.csv\")\n",
        "        #!gsutil cp /content/Aladag_labeled_preprocessed.csv gs://{bucket_name}/Aladag_labeled_preprocessed.csv\n",
        "\n",
        "    else:\n",
        "\n",
        "        output.to_csv(\"Aladag_sample_preprocessed.csv\")\n",
        "        #!gsutil cp /content/Aladag_sample_preprocessed.csv gs://{bucket_name}/Aladag_sample_preprocessed.csv\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "elif(eight_classes==True and preprocessing_done == False):\n",
        "\n",
        "\n",
        "    output = output[output['posts'].str.split().str.len().gt(10)]\n",
        "\n",
        "\n",
        "    output = preprocess_text(output)\n",
        "\n",
        "\n",
        "    if labaled_data:\n",
        "        output.to_csv(\"df_mental_valid_preprocessed.csv\")\n",
        "        #Its commented out so that I accidentaly dont overwrite my files \n",
        "        #!gsutil cp /content/df_mental_valid_preprocessed.csv gs://{bucket_name}/df_mental_valid_preprocessed.csv\n",
        "\n",
        "    else:\n",
        "        output.to_csv(\"df_mental_balanced_preprocessed.csv\")\n",
        "        #Its commented out so that I accidentaly dont overwrite my files \n",
        "\n",
        "        #!gsutil cp /content/df_mental_balanced_preprocessed.csv gs://{bucket_name}/df_mental_balanced_preprocessed.csv\n",
        "\n",
        "    \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZJ58phy514A"
      },
      "outputs": [],
      "source": [
        "labaled_data=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPUzN8TLti_4"
      },
      "outputs": [],
      "source": [
        "if preprocessing_done:\n",
        "  \n",
        "    if labaled_data:\n",
        "        if aladag:\n",
        "            output = pd.read_csv(\"Aladag_labeled_preprocessed.csv\")\n",
        "\n",
        "        elif eight_classes and preprocessing_done_final==False:\n",
        "\n",
        "            output = pd.read_csv(\"df_mental_valid_preprocessed.csv\")\n",
        "\n",
        "            output = output.astype({\"text\": str, \"label\":str}, errors='raise')\n",
        "            output = output[output.label != \"nan\"]\n",
        "            output.reset_index(drop=True, inplace=True)\n",
        "            column = \"text\"\n",
        "            output = preprocess_text(output)\n",
        "\n",
        "\n",
        "        elif reddit_500:\n",
        "\n",
        "\n",
        "            dftest = pd.read_csv(\"reddit_500_final_val.csv\")\n",
        "\n",
        "        elif rsdd:\n",
        "\n",
        "            output = pd.read_csv(\"rsdd_valid_preprocessed.csv\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        if aladag:\n",
        "\n",
        "            output = pd.read_csv(\"Aladag_sample_preprocessed.csv\")\n",
        "\n",
        "        elif eight_classes and preprocessing_done_final==False:\n",
        "\n",
        "            output = pd.read_csv(\"df_mental_balanced_preprocessed.csv\")\n",
        "\n",
        "            output = output.astype({\"text\": str, \"label\":str}, errors='raise')\n",
        "            output = output[output.label != \"nan\"]\n",
        "            output.reset_index(drop=True, inplace=True)\n",
        "            column = \"text\"\n",
        "            output = preprocess_text(output)\n",
        "\n",
        "        elif reddit_500:\n",
        "\n",
        "\n",
        "            df = pd.read_csv(\"reddit_500_final_train.csv\")\n",
        "\n",
        "        elif rsdd:\n",
        "\n",
        "            output = pd.read_csv(\"rsdd_train_preprocessed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g37xnKv8plr"
      },
      "outputs": [],
      "source": [
        "preprocessing_done_final=True\n",
        "labaled_data = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJTaQ7qW7hJW"
      },
      "outputs": [],
      "source": [
        "if preprocessing_done_final:\n",
        "    if eight_classes:\n",
        "\n",
        "        if labaled_data:\n",
        "        \n",
        "            output = pd.read_csv(\"df_mental_valid_preprocessed_preprocessed.csv\")\n",
        "\n",
        "        else:\n",
        "\n",
        "            output = pd.read_csv(\"df_mental_balanced_preprocessed_preprocessed.csv\")\n",
        "\n",
        "else:\n",
        "    if preprocessing_done: \n",
        "\n",
        "\n",
        "        if labaled_data:\n",
        "            if eight_classes:\n",
        "              output.to_csv(\"df_mental_valid_preprocessed_preprocessed.csv\")\n",
        "              #Its commented out just not to accidentally overwrite              \n",
        "              #!gsutil cp  /content/df_mental_valid_preprocessed_preprocessed.csv gs://masterthesisbert/df_mental_valid_preprocessed_preprocessed.csv \n",
        "        else: \n",
        "            if eight_classes:\n",
        "              output.to_csv(\"df_mental_balanced_preprocessed_preprocessed.csv\")\n",
        "              #Its commented out just not to accidentally overwrite              \n",
        "\n",
        "\n",
        "              #!gsutil cp  /content/df_mental_balanced_preprocessed_preprocessed.csv gs://masterthesisbert/df_mental_balanced_preprocessed_preprocessed.csv "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoXVw_f7wwds"
      },
      "outputs": [],
      "source": [
        "if aladag:\n",
        "    output = output.rename(columns={'selftext': 0, 'binary_annotation': 1})\n",
        "\n",
        "elif eight_classes:\n",
        "\n",
        "    output = output.rename(columns={'text': 0, 'label': 1})\n",
        "    final_table_columns = [0, 1]\n",
        "    output = output[output.columns.intersection(final_table_columns)]\n",
        "    output=output.dropna(axis=0)\n",
        "    len(output)\n",
        "\n",
        "elif reddit_500:\n",
        "\n",
        "    df = df.rename(columns={'selftext': 0, 'Label': 1})\n",
        "    dftest = dftest.rename(columns={'selftext': 0, 'Label': 1})\n",
        "    final_table_columns = [0, 1]\n",
        "    df = df[df.columns.intersection(final_table_columns)]\n",
        "    dftest = dftest[dftest.columns.intersection(final_table_columns)]\n",
        "    df=df.dropna(axis=0)\n",
        "    dftest=dftest.dropna(axis=0)\n",
        "\n",
        "elif rsdd:\n",
        "\n",
        "    output = output.rename(columns={'posts': 0, 'label': 1})\n",
        "    final_table_columns = [0, 1]\n",
        "    output = output[output.columns.intersection(final_table_columns)]\n",
        "    output=output.dropna(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8agY0Sn80Ahm"
      },
      "outputs": [],
      "source": [
        "if reddit_500:\n",
        "  df_train_x = df.copy()\n",
        "  df_test_x = dftest.copy()\n",
        "\n",
        "else:\n",
        "  df_train_x = output.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "HTT2J30EH8nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c25m1Mhvo9Xp"
      },
      "outputs": [],
      "source": [
        "def text_to_embeddings(df_train_x, tokenizer, model, start_from = 0):\n",
        "    \n",
        "    max_features = 256\n",
        "    big_step = 1000\n",
        "    if start_from == 0:\n",
        "        a = 0\n",
        "    else: \n",
        "        a = start_from/big_step\n",
        "    b = len(df_train_x)\n",
        "    for j in list(range(start_from,b,big_step)):\n",
        "        print(j, \"j\")\n",
        "        if (b - j) < big_step:\n",
        "            # for the last step\n",
        "            start = j\n",
        "            stop = b\n",
        "            df_short = df_train_x[start:stop].copy()\n",
        "            print(\"last shot\")\n",
        "            bert_features_first_2D, bert_features_first_3D  = getFeatures(df_short, tokenizer, model)\n",
        "        else:\n",
        "            for i in list(range(j,j + big_step,100)):\n",
        "                print(i, \"i\")\n",
        "                if ((i == 0) or (i%big_step==0)):\n",
        "                    print(\"starting\")\n",
        "                    start = i\n",
        "                    stop = i + 100\n",
        "                    df_short = df_train_x[start:stop].copy()\n",
        "                    bert_features_first_2D, bert_features_first_3D = getFeatures(df_short, tokenizer, model)                \n",
        "                elif i == list(range(j,j+big_step+1,100))[-1]:\n",
        "                    #do nothing\n",
        "                    print(\"we did nothing\")\n",
        "                    '''    \n",
        "                    elif i == list(range(j,j+big_step,100))[-1]:\n",
        "                    start = i\n",
        "                    stop = len(df_train_x)\n",
        "                    df_short = df_train_x[start:stop].copy()\n",
        "                    bert_features_third= getFeatures(df_short, tokenizer, model)\n",
        "                    bert_features_third = np.concatenate((bert_features_third, bert_features_first), axis=0)\n",
        "                    '''\n",
        "                else:\n",
        "                    print(\"next step\")\n",
        "                    start = i\n",
        "                    stop = i + 100\n",
        "                    df_short_2 = df_train_x[start:stop].copy()\n",
        "                    bert_features_second_2D, bert_features_second_3D =  getFeatures(df_short_2, tokenizer, model)\n",
        "\n",
        "                    bert_features_first_2D = np.concatenate((bert_features_first_2D, bert_features_second_2D), axis=0)\n",
        "         \n",
        "                    bert_features_first_3D = np.concatenate((bert_features_first_3D, bert_features_second_3D), axis=0)\n",
        "\n",
        "        name_3D = f\"MyBERT_wide_full-training-features_3D_labeled_rsdd_val{a}.txt\"\n",
        "\n",
        "        name_2D = f\"MyBERT_wide_full-training-features_2D_labeled_rsdd_val{a}.txt\"\n",
        "        \n",
        "        # reshaping the array from 3D\n",
        "        # matrice to 2D matrice.\n",
        "        bert_features_first_3D = bert_features_first_3D.reshape(bert_features_first_3D.shape[0], -1)\n",
        "  \n",
        "        # saving reshaped array to file.\n",
        "        \n",
        "        np.savetxt(name_2D, bert_features_first_2D)\n",
        "        np.savetxt(name_3D, bert_features_first_3D)\n",
        "\n",
        "\n",
        "        !gsutil cp /content/{name_2D} gs://masterthesisbert/embeddings_labeled/PsychBERT_long/\n",
        "        !gsutil cp /content/{name_3D} gs://masterthesisbert/embeddings_labeled/PsychBERT_long/\n",
        "\n",
        "\n",
        "        a+=1 \n",
        "    return bert_features_first_2D, bert_features_first_3D\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_train_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6NhFTCctYmU",
        "outputId": "58b235cd-2865-432b-a2f8-c2319238b254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EfXd7ms3EyG"
      },
      "outputs": [],
      "source": [
        "#for reddit_500 change df_train_x to df_val_x \n",
        "\n",
        "bert_features_train_2D, bert_features_train_3D = text_to_embeddings(df_train_x, tokenizer, model)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
