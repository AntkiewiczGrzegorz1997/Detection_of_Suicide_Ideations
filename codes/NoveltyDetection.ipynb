{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krI5XYEpFAr_"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install \"tensorflow_hub>=0.6.0\"\n",
        "!pip3 install tensorflow_text==1.15\n",
        "!pip install transformers\n",
        "!pip install numpy==1.19.5\n",
        "!pip install 'h5py==2.10.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3vcuyxyf3RY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers as ppb\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import natsort\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, MaxPooling1D, Dropout, Conv1D, Input, LSTM, SpatialDropout1D, Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwP8abYbB3CG"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntmkOLyWJm8I"
      },
      "outputs": [],
      "source": [
        "aladag = False\n",
        "reddit_500 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFoAxlsy5pTW"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "print(h5py.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IF the numpy version not equal to 1.19.5 and h5py version not equal to 2.10.0 restart the runtime and start again from cell 2"
      ],
      "metadata": {
        "id": "skcphLLGK_ZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XwUXELFH5Rb"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!curl https://sdk.cloud.google.com | bash\n",
        "\n",
        "!gcloud init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPa7DkOw00g3"
      },
      "outputs": [],
      "source": [
        "!mkdir training_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyg5hZxV5KtF"
      },
      "outputs": [],
      "source": [
        "#Choose the path to the dataset and embeddings\n",
        "!gsutil -m cp gs://masterthesisbert/embeddings_right/PsychBERT_con_long/MyBERT_wide_full-training-features_2D_labeled_Alada* /content/training_directory/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RO3zrPHBm5r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import natsort\n",
        "arr = os.listdir(\"/content/training_directory\")\n",
        "arr = natsort.natsorted(arr)\n",
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpIKD1PW7bvh"
      },
      "outputs": [],
      "source": [
        "# load numpy array from csv file\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "\n",
        "third_dimension = 768\n",
        "\n",
        "arrays = []\n",
        "\n",
        "if aladag:\n",
        "\n",
        "    length = 10\n",
        "\n",
        "elif reddit_500:\n",
        "\n",
        "    length = 5\n",
        "\n",
        "for i in range(len(arr[0:length])):\n",
        "    loaded_arr = np.loadtxt('/content/training_directory'+\"/\"+arr[i])\n",
        "    arrays.append(loaded_arr)\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf-ql0nZV9ch"
      },
      "outputs": [],
      "source": [
        "concatenated_array = np.array([]).reshape(0, 768)\n",
        "# Create an array to return to\n",
        "\n",
        "for array in arrays:\n",
        "\n",
        "    concatenated_array = np.concatenate([concatenated_array, array], axis=0)\n",
        "data_train_x = concatenated_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak893ebkJCwk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "if aladag:  \n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_sample_preprocessed.csv  /content/Aladag_sample_preprocessed.csv\n",
        "    \n",
        "    df_name = \"/content/Aladag_sample_preprocessed.csv\"\n",
        "    df = pd.read_csv(df_name)\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_labeled_preprocessed.csv  /content/Aladag_labeled_preprocessed.csv\n",
        "\n",
        "    df_name2 = \"/content/Aladag_labeled_preprocessed.csv\"\n",
        "    df2 = pd.read_csv(df_name2)\n",
        "    df2\n",
        "\n",
        "    train_y_tensor = torch.tensor(df[0:10000].binary_annotation.values)\n",
        "    train_y_tensor2 = torch.tensor(df2[0:10000].binary_annotation.values)\n",
        "\n",
        "elif reddit_500:\n",
        "  \n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_val.csv  /content/reddit_500_final_val.csv\n",
        "\n",
        "    df_name2 = \"/content/reddit_500_final_val.csv\"\n",
        "    df2 = pd.read_csv(df_name2)\n",
        "    df2\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_train.csv  /content/reddit_500_final_train.csv\n",
        "\n",
        "    df_name = \"/content/reddit_500_final_train.csv\"\n",
        "    df = pd.read_csv(df_name)\n",
        "    df\n",
        "\n",
        "    train_y_tensor = torch.tensor(df[0:10000].Label.values)\n",
        "    train_y_tensor2 = torch.tensor(df2[0:10000].Label.values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "permutation_done = True\n",
        "if permutation_done and reddit_500:\n",
        "    !gsutil cp gs://masterthesisbert/parameter_optimization/permutation.csv  /content/permutation.csv\n",
        "    from numpy import loadtxt\n",
        "    p = loadtxt('permutation.csv', delimiter=',')\n",
        "    concatenated_array, train_y_tensor_permut = data_train_x[p.astype(int)], train_y_tensor[p.astype(int)]\n",
        "    data_train_x, train_y_tensor_permut = data_train_x[p.astype(int)], train_y_tensor[p.astype(int)]"
      ],
      "metadata": {
        "id": "CtDXou9S81NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIXnfZojIBhu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.layers import Input, Dense,RepeatVector, TimeDistributed, Dense, Dropout, LSTM, Bidirectional\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSTSuA7b5HT0"
      },
      "source": [
        "#Dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybRdAaPfROl5"
      },
      "outputs": [],
      "source": [
        "def summarize_results(grid_result, model_name, result_to_file = True):\n",
        "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "    means = grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "    for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "    if result_to_file == True:\n",
        "        string_to_save = f\"Best: {grid_result.best_score_} using {grid_result.best_params_} \\n\" \n",
        "        for mean, stdev, param in zip(means, stds, params):\n",
        "            string_to_save = string_to_save + f\"{mean} ({stdev}) with: {param} \\n\" \n",
        "        text_results = open(model_name + \".txt\", \"w\")\n",
        "        n = text_results.write(string_to_save)\n",
        "        text_results.close()\n",
        "\n",
        "from tensorflow.keras.models import model_from_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIkMu3BsUxcg"
      },
      "outputs": [],
      "source": [
        "# Conctruct autoencoder for training test to find anomalous days in the data\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "import random as rn \n",
        "from tensorflow.compat.v1.keras import backend as K\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "number = 2\n",
        "os.environ['PYTHONHASHSEED']=str(number)\n",
        "#Set random seed for numpy, python and tensorflow\n",
        "np.random.seed(number)\n",
        "rn.seed(number)\n",
        "tf.set_random_seed(number)\n",
        "# Set the number of threads to 1 \n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cICSTLsP-Hb"
      },
      "source": [
        "#Novelty detection approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cXwg3xMP9B7"
      },
      "outputs": [],
      "source": [
        "def create_dense_autoencoder(data_train_x = data_train_x):\n",
        "\n",
        "    # Fully Dense Network\n",
        "    model_autoencoder_dense = Sequential()\n",
        "\n",
        "    model_autoencoder_dense_path = \"model_autoencoder_dense\"\n",
        "\n",
        "    #model_autoencoder_dense.add(Input(shape=(data_train_x.shape[1], data_train_x.shape[2],)))\n",
        "\n",
        "    model_autoencoder_dense.add(Dense(768/2, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number), input_shape=(data_train_x.shape[1],)))\n",
        "    model_autoencoder_dense.add(Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "    model_autoencoder_dense.add(Dense(768/2, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "    model_autoencoder_dense.add(Dense(768, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "    #model_autoencoder_dense.add(keras.layers.TimeDistributed(keras.layers.Dense(data_train_x.shape[2],kernel_initializer=keras.initializers.glorot_uniform(seed=number) )))\n",
        "        \n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.00001)\n",
        "\n",
        "\n",
        "    model_autoencoder_dense.compile(optimizer=optimizer,\n",
        "                  loss='mse')\n",
        "\n",
        "    #mc = ModelCheckpoint(model_autoencoder_dense_path + \".h5\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "    model_autoencoder_dense.summary()\n",
        "\n",
        "    return model_autoencoder_dense\n",
        "\n",
        "model_autoencoder_Dense = KerasRegressor(build_fn=create_dense_autoencoder, epochs=10)\n",
        "param_grid = dict(epochs=[120])\n",
        "grid = GridSearchCV(estimator=model_autoencoder_Dense, param_grid=param_grid, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ9UbhByPuJl"
      },
      "outputs": [],
      "source": [
        "model_autoencoder_dense = create_dense_autoencoder(data_train_x)\n",
        "#choose the best parameters as from the grid search \n",
        "model_autoencoder_dense.fit(data_train_x, data_train_x, epochs=120)\n",
        "\n",
        "model_autoencoder_dense.save('model_autoencoder_dense.h5')\n",
        "new_model = tf.keras.models.load_model('model_autoencoder_dense.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMGd6aEiUoxW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "X_pred = model_autoencoder_dense.predict(data_train_x)\n",
        "\n",
        "#obtain mean square error for each day between predicted 24 hours and actual 24 hours \n",
        "\n",
        "train_mse_loss = np.mean(np.square(X_pred - data_train_x), axis=1)\n",
        "\n",
        "train_mse_loss.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEFRSCvz3wU_"
      },
      "outputs": [],
      "source": [
        "import skimage.io\n",
        "import skimage.color\n",
        "import skimage.filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geFfykYT25z1",
        "outputId": "a8793c4c-baed-47d7-e526-b5db37703644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found automatic threshold t = 0.11656135659109973.\n"
          ]
        }
      ],
      "source": [
        "# perform automatic thresholding\n",
        "t = skimage.filters.threshold_otsu(train_mse_loss)\n",
        "print(\"Found automatic threshold t = {}.\".format(t)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JYV2qIBxmhl"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "#3error_sum = np.sum(train_mse_loss, axis=1)\n",
        "plt.hist(train_mse_loss, bins=80);\n",
        "plt.axvline(t, color='red', linestyle='dashed', linewidth=1)\n",
        "plt.ylabel('NofObservations')\n",
        "#plt.ylim(y_min, y_max)\n",
        "plt.xlabel('MSE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxIJF0phUtMm",
        "outputId": "b8b00884-8cd0-461a-8188-759652bad6db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4818, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ],
      "source": [
        "anomaly_day_indices = [idx for idx, val in enumerate(train_mse_loss) if val > t]\n",
        "full_train_without_anomalies = np.delete(data_train_x, anomaly_day_indices, axis=0)\n",
        "\n",
        "# Data is ready for a Classification\n",
        "full_train_without_anomalies.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYT9VAOWWUYf"
      },
      "outputs": [],
      "source": [
        "# Fully Dense Network\n",
        "\n",
        "model_autoencoder_dense_path2 = \"model_autoencoder_dense2\"\n",
        "inputs = tf.keras.Input(shape=(data_train_x.shape[1],))\n",
        "encoder_1 = Dense(data_train_x.shape[1]/2, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number))(inputs)\n",
        "\n",
        "encoder_2 = Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number))(encoder_1)\n",
        "\n",
        "decoder_1 = Dense(data_train_x.shape[1]/2, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed=number))(encoder_2)\n",
        "\n",
        "decoder_2 = Dense(data_train_x.shape[1], activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed=number))(decoder_1)\n",
        "\n",
        "\n",
        "model_autoencoder_dense2 = Model(inputs = inputs, outputs = decoder_2)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00001)\n",
        "\n",
        "model_autoencoder_dense2.compile(loss=\"mse\", optimizer=optimizer)\n",
        "                                 \n",
        "\n",
        "#mc = ModelCheckpoint(model_autoencoder_dense_path + \".h5\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_autoencoder_dense2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g-evbh-WiG_"
      },
      "outputs": [],
      "source": [
        "model_autoencoder_dense2.fit(full_train_without_anomalies, full_train_without_anomalies, epochs=120)\n",
        "\n",
        "model_autoencoder_dense2.save('model_autoencoder_dense2.h5')\n",
        "model_autoencoder_dense2= tf.keras.models.load_model('model_autoencoder_dense2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjW0vrvOrlLj"
      },
      "outputs": [],
      "source": [
        "from google.colab import files \n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2Lkj8_woanK"
      },
      "outputs": [],
      "source": [
        "def get_normal_rows(data_train_x, model_autoencoder, inputs, encoder_2, eps=0.005, shape=10000):\n",
        "\n",
        "    X_pred = model_autoencoder.predict(data_train_x)\n",
        "    #obtain mean square error for each day between predicted 24 hours and actual 24 hours \n",
        "    train_mse_loss_final = np.mean(np.square(X_pred - data_train_x), axis=1)\n",
        "\n",
        "    # perform automatic thresholding\n",
        "    t_2 = skimage.filters.threshold_otsu(train_mse_loss_final)\n",
        "    print(\"Found automatic threshold t = {}.\".format(t_2)) \n",
        "\n",
        "    #plot a histogram\n",
        "    #3error_sum = np.sum(train_mse_loss, axis=1)\n",
        "    plt.hist(train_mse_loss_final, bins=80);\n",
        "    plt.axvline(t_2, color='red', linestyle='dashed', linewidth=1)\n",
        "\n",
        "    plt.ylabel('NofObservations')\n",
        "    #plt.ylim(y_min, y_max)\n",
        "    plt.xlabel('MSE')\n",
        "\n",
        "    encoder = Model(inputs , encoder_2)\n",
        "    encoded_train = encoder.predict(data_train_x)\n",
        "    encoded_train_reshaped = encoded_train.reshape(shape, 1)\n",
        "    plt.scatter(train_mse_loss_final, encoded_train)\n",
        "    plt.show()\n",
        "    merged_list = []\n",
        "\n",
        "    for l in encoded_train:\n",
        "        \n",
        "        merged_list.append(l[0])\n",
        "\n",
        "    merged_list_new = numpy.array(merged_list)\n",
        "    merged_list_new.shape\n",
        "    Z = np.dstack((train_mse_loss_final, merged_list_new))\n",
        "    merged_list_2 = []\n",
        "\n",
        "    for l in range(len(Z[0])):\n",
        "        \n",
        "        merged_list_2.append(Z[0][l])\n",
        "    #Clustering\n",
        "    nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(merged_list_2)\n",
        "    distances, indices = nbrs.kneighbors(merged_list_2)\n",
        "\n",
        "    sns.set()\n",
        "    distances = np.sort(distances, axis=0)\n",
        "    distances = distances[:,1]\n",
        "    plt.plot(distances)\n",
        "    #The optimal point for eps will the found at the point of maximal curvature\n",
        "    \n",
        "    m = DBSCAN(eps, min_samples=5)\n",
        "    m.fit(merged_list_2)\n",
        "    clusters = m.labels_\n",
        "    # taking an input list\n",
        "    l1 = []\n",
        "    \n",
        "    # taking an counter\n",
        "    n_clusters = 0\n",
        "   \n",
        "    # traversing the array\n",
        "    for item in clusters:\n",
        "        if item not in l1:\n",
        "            n_clusters += 1\n",
        "            l1.append(item)\n",
        "    \n",
        "    # printing the output\n",
        "    print(\"No of unique items are:\", n_clusters)\n",
        "\n",
        "    def find(lst, a):\n",
        "        return [i for i, x in enumerate(lst) if x==a]\n",
        "    dict_error = {}\n",
        "    cluster_labels = list(range(-1, n_clusters-1))\n",
        "\n",
        "    for i in range(-1, n_clusters-1):\n",
        "        cluster_i_set = []\n",
        "        for k in find(clusters, i):\n",
        "            cluster_i_set.append(merged_list_2[k])\n",
        "        #cluster_i_set = merged_list_2[find(clusters, i)]\n",
        "        len_cluster_i_set = len(cluster_i_set)\n",
        "        error_count = 0\n",
        "        for j in range(len_cluster_i_set):\n",
        "            if cluster_i_set[j][0]>t_2:\n",
        "              error_count += 1\n",
        "\n",
        "        dict_error[i] = error_count\n",
        "    occurences_of_cluster = {}\n",
        "    novelty_normal = {}\n",
        "\n",
        "    def return_all_normal_indices(clusters, key):\n",
        "        return [i for i, e in enumerate(clusters.tolist()) if e == key]\n",
        "\n",
        "    for i in range(-1, n_clusters-1):\n",
        "        number = clusters.tolist().count(i)\n",
        "        if (number * 0.7 <= dict_error[i] ):\n",
        "            novelty_normal[i] = \"novelty\"\n",
        "        else:\n",
        "            novelty_normal[i] = \"normal\"\n",
        "    normal_clusters = []\n",
        "    normal_indices = []\n",
        "    for key, item in novelty_normal.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
        "        if item == \"normal\":\n",
        "            normal_clusters.append(key)\n",
        "            normal_indices = normal_indices + return_all_normal_indices(clusters, key)\n",
        "\n",
        "    return normal_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4TCZ8PopUfC"
      },
      "outputs": [],
      "source": [
        "normal_indices = get_normal_rows(data_train_x, model_autoencoder_dense2, inputs, encoder_2, shape = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqURzORXqI4S"
      },
      "outputs": [],
      "source": [
        "len(normal_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSIuQ6SN-axQ"
      },
      "outputs": [],
      "source": [
        "# open file in write mode. Actually the name of the embeddigs and the dataset would be cool. \n",
        "with open(r'BERT_reddit_500_noisy1.txt', 'w') as fp:\n",
        "    for item in normal_indices:\n",
        "        # write each item on a new line\n",
        "        fp.write(\"%s\\n\" % item)\n",
        "    print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzYi6-b8_pM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6065a607-5565-4299-b2e2-65a78fd9a6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/BERT_reddit_500_noisy1.txt [Content-Type=text/plain]...\n",
            "/ [1 files][ 18.3 KiB/ 18.3 KiB]                                                \n",
            "Operation completed over 1 objects/18.3 KiB.                                     \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp /content/BERT_reddit_500_noisy1.txt gs://masterthesisbert/anomaly_indices/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NoveltyDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
