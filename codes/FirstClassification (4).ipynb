{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNCBkC2mSxbr"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install \"tensorflow_hub>=0.6.0\"\n",
        "!pip3 install tensorflow_text==1.15\n",
        "!pip install transformers\n",
        "!pip install numpy==1.19.5\n",
        "!pip install 'h5py==2.10.0'\n",
        "!pip install natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6yKx0hTTI-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers as ppb\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import natsort\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, MaxPooling1D, Dropout, Conv1D, Input, LSTM, SpatialDropout1D, Bidirectional, Conv1D, MaxPooling2D, Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i0WTGYMTfEG"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZPgo-iiTiAQ"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "print(h5py.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHKdTsZK5Hnq"
      },
      "source": [
        "#IF the numpy version not equal to 1.19.5 and h5py version not equal to 2.10.0 restart the runtime and start again from cell 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQXj6VqETk8A"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!curl https://sdk.cloud.google.com | bash\n",
        "\n",
        "\n",
        "!gcloud init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s52AH13FGsz9"
      },
      "outputs": [],
      "source": [
        "validation = True\n",
        "gridsearch=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R87U1jCgUccx"
      },
      "outputs": [],
      "source": [
        "!mkdir training_directory\n",
        "!mkdir validation_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo_2RbBuUcfC"
      },
      "outputs": [],
      "source": [
        "#choose the option depending on what is needed\n",
        "if validation:\n",
        "\n",
        "    #!gsutil -m cp gs://masterthesisbert/embeddings_labeled/PsychBERT_con_long/MyBERT_wide_full-training-features_2D_labeled_reddit_500_val* /content/validation_directory/\n",
        "    #!gsutil -m cp gs://masterthesisbert/embeddings_right/PsychBERT_con_long/MyBERT_wide_full-training-features_2D_labeled_reddit_500_train* /content/training_directory/\n",
        "    !gsutil -m cp gs://masterthesisbert/embeddings_labeled/BERT/MyBERT_wide_full-training-features_3D_labeled_Aladag* /content/validation_directory/\n",
        "    !gsutil -m cp gs://masterthesisbert/embeddings_right/BERT/BERT_wide_full-training-features_3D_labeled_Aladag* /content/training_directory/\n",
        "\n",
        "else:\n",
        "\n",
        "    #!gsutil -m cp gs://masterthesisbert/embeddings_right/PsychBERT_con_long/MyBERT_wide_full-training-features_3D_labeled_Alada* /content/training_directory/\n",
        "    #!gsutil -m cp gs://masterthesisbert/embeddings_right/PsychBERT_con_long/MyBERT_wide_full-training-features_2D_labeled_Alada* /content/training_directory/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C84SNKjSUchT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if validation:\n",
        "    arr = os.listdir(\"/content/training_directory\")\n",
        "    arr = natsort.natsorted(arr)\n",
        "    print(arr)\n",
        "    arrval = os.listdir(\"/content/validation_directory\")\n",
        "    arrval = natsort.natsorted(arrval)\n",
        "    print(arrval)\n",
        "\n",
        "else:\n",
        "    arr = os.listdir(\"/content/training_directory\")\n",
        "    arr = natsort.natsorted(arr)\n",
        "    print(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS_BT925EsTQ"
      },
      "outputs": [],
      "source": [
        "# load numpy array from csv file\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "\n",
        "third_dimension = 768\n",
        "\n",
        "arrays = []\n",
        "arrays2 = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aladag = True\n",
        "reddit_500 = False\n",
        "rsdd = False\n",
        "ten_classes = False"
      ],
      "metadata": {
        "id": "DupdeiLLS7-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if aladag:\n",
        "\n",
        "    length = 10\n",
        "    length_1 = 1\n",
        "elif reddit_500:\n",
        "    length = 5\n",
        "    length_1 = 2\n",
        "\n",
        "elif rsdd:\n",
        "    length = 5\n",
        "    length_1 = 2\n",
        "\n",
        "elif ten_classes:\n",
        "    length = 10\n",
        "    length_1 = 2"
      ],
      "metadata": {
        "id": "vufJtcmHTD3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeGu8uu-Uclm"
      },
      "outputs": [],
      "source": [
        "if(validation==True):\n",
        "    for i in range(len(arr[0:length])):\n",
        "        print(arr[i])\n",
        "        loaded_arr = np.loadtxt('/content/training_directory'+\"/\"+arr[i])\n",
        "        arrays.append(loaded_arr)\n",
        "        print(i)\n",
        "    \n",
        "    for i in range(length_1):\n",
        "      print(arrval[i])\n",
        "      loaded_arr2 = np.loadtxt('/content/validation_directory'+\"/\"+arrval[i])\n",
        "      arrays2.append(loaded_arr2)\n",
        "      print(i)\n",
        "    \n",
        "else:\n",
        "    \n",
        "    #choose the right one for validation\n",
        "    for i in range(len(arr[0:length])):\n",
        "       \n",
        "        loaded_arr = np.loadtxt('/content/training_directory'+\"/\"+arr[i])\n",
        "        arrays.append(loaded_arr)\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP_s38fGUcnn"
      },
      "outputs": [],
      "source": [
        "threeD = True\n",
        "if threeD:\n",
        "    \n",
        "    concatenated_array = np.array([]).reshape(0, arrays[2].shape[1])\n",
        "\n",
        "    for array in arrays:\n",
        "\n",
        "        concatenated_array = np.concatenate([concatenated_array, array], axis=0)\n",
        "\n",
        "    del arrays\n",
        "\n",
        "    load_original_arr = concatenated_array.reshape(\n",
        "        concatenated_array.shape[0], concatenated_array.shape[1] // third_dimension, third_dimension)\n",
        "    del concatenated_array\n",
        "    data_train_x = load_original_arr\n",
        "    del load_original_arr\n",
        "\n",
        "\n",
        "    \n",
        "    concatenated_array2 = np.array([]).reshape(0, arrays2[0].shape[1])\n",
        "\n",
        "    for array in arrays2:\n",
        "\n",
        "        concatenated_array2 = np.concatenate([concatenated_array2, array], axis=0)\n",
        "\n",
        "    del arrays2\n",
        "\n",
        "    load_original_arr2 = concatenated_array2.reshape(\n",
        "        concatenated_array2.shape[0], concatenated_array2.shape[1] // third_dimension, third_dimension)\n",
        "    del concatenated_array2\n",
        "    data_train_x2 = load_original_arr2\n",
        "\n",
        "else:\n",
        "\n",
        "    if(validation==False):\n",
        "\n",
        "        concatenated_array = np.array([]).reshape(0, 768)\n",
        "        # Create an array to return to\n",
        "\n",
        "        for array in arrays:\n",
        "\n",
        "            concatenated_array = np.concatenate([concatenated_array, array], axis=0)\n",
        "        data_train_x = concatenated_array\n",
        "    else: \n",
        "        concatenated_array = np.array([]).reshape(0, 768)\n",
        "        # Create an array to return to\n",
        "\n",
        "        for array in arrays:\n",
        "\n",
        "            concatenated_array = np.concatenate([concatenated_array, array], axis=0)\n",
        "        data_train_x = concatenated_array\n",
        "\n",
        "        concatenated_array2 = np.array([]).reshape(0, 768)\n",
        "        # Create an array to return to\n",
        "\n",
        "        for array in arrays2:\n",
        "\n",
        "            concatenated_array2 = np.concatenate([concatenated_array2, array], axis=0)\n",
        "        data_train_x2 = concatenated_array2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9DONEEGUcr5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "if aladag:  \n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_sample_preprocessed.csv  /content/Aladag_sample_preprocessed.csv\n",
        "    \n",
        "    df_name = \"/content/Aladag_sample_preprocessed.csv\"\n",
        "    df = pd.read_csv(df_name)\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_labeled_preprocessed.csv  /content/Aladag_labeled_preprocessed.csv\n",
        "\n",
        "    df_name2 = \"/content/Aladag_labeled_preprocessed.csv\"\n",
        "    df2 = pd.read_csv(df_name2)\n",
        "    df2\n",
        "\n",
        "    train_y_tensor = torch.tensor(df[0:10000].binary_annotation.values)\n",
        "    train_y_tensor2 = torch.tensor(df2[0:10000].binary_annotation.values)\n",
        "\n",
        "\n",
        "elif reddit_500:\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_val.csv  /content/reddit_500_final_val.csv\n",
        "\n",
        "    df_name2 = \"/content/reddit_500_final_val.csv\"\n",
        "    df2 = pd.read_csv(df_name2)\n",
        "    df2\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_train.csv  /content/reddit_500_final_train.csv\n",
        "\n",
        "    df_name = \"/content/reddit_500_final_train.csv\"\n",
        "    df = pd.read_csv(df_name)\n",
        "    df\n",
        "\n",
        "    train_y_tensor = torch.tensor(df[0:10000].Label.values)\n",
        "    train_y_tensor2 = torch.tensor(df2[0:10000].Label.values)\n",
        "\n",
        "elif rsdd:\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/rsdd_valid_preprocessed.csv  /content/rsdd_valid_preprocessed.csv\n",
        "\n",
        "    df_name2 = \"/content/rsdd_valid_preprocessed.csv\"\n",
        "    df2 = pd.read_csv(df_name2)\n",
        "    df2\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/rsdd_train_preprocessed.csv  /content/rsdd_train_preprocessed.csv\n",
        "\n",
        "    df_name = \"/content/rsdd_train_preprocessed.csv\"\n",
        "    df = pd.read_csv(df_name)\n",
        "\n",
        "    train_y_tensor = torch.tensor(df[0:5000].label.values)\n",
        "    train_y_tensor2 = torch.tensor(df2[0:2000].label.values)\n",
        "\n",
        "\n",
        "elif ten_classes:\n",
        "\n",
        "    \n",
        "    !gsutil cp gs://masterthesisbert/df_mental_valid_preprocessed_preprocessed.csv  /content/df_mental_valid_preprocessed_preprocessed.csv\n",
        "\n",
        "    df_name2 = \"/content/df_mental_valid_preprocessed_preprocessed.csv\"\n",
        "    df2 = pd.read_csv(df_name2)\n",
        "    df2\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/df_mental_balanced_preprocessed_preprocessed.csv  /content/df_mental_balanced_preprocessed_preprocessed.csv \n",
        "    df_name = \"/content/df_mental_balanced_preprocessed_preprocessed.csv\"\n",
        "    df = pd.read_csv(df_name)\n",
        "    df\n",
        "    \n",
        "    df.cc = pd.Categorical(df.label)\n",
        "    df[\"cat\"] = df.cc.codes\n",
        "    df2.cc = pd.Categorical(df2.label)\n",
        "    df2[\"cat\"] = df2.cc.codes  \n",
        "    train_y_tensor = torch.tensor(df.cat.values)     \n",
        "        \n",
        "    train_y_tensor2 = torch.tensor(df2.cat.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUd4wNY4bUpj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.layers import Input, Dense,RepeatVector, TimeDistributed, Dense, Dropout, LSTM, Bidirectional\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCMawPI9biwI"
      },
      "outputs": [],
      "source": [
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "991CzKDNcg3T"
      },
      "outputs": [],
      "source": [
        "# Conctruct autoencoder for training test to find anomalous days in the data\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "import random as rn \n",
        "from tensorflow.compat.v1.keras import backend as K\n",
        "number = 2\n",
        "os.environ['PYTHONHASHSEED']=str(number)\n",
        "#Set random seed for numpy, python and tensorflow\n",
        "np.random.seed(number)\n",
        "rn.seed(number)\n",
        "tf.set_random_seed(number)\n",
        "# Set the number of threads to 1 \n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "# Its an example of an autoencoder. I might use it later. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afpjlaSmyd7L"
      },
      "outputs": [],
      "source": [
        "def unison_shuffled_copies(a, b, just_indices=False):\n",
        "        assert len(a) == len(b)\n",
        "        p = numpy.random.permutation(len(a))\n",
        "        return a[p], b[p], p "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL_rZRtuvzAh"
      },
      "outputs": [],
      "source": [
        "permutation_done = True\n",
        "\n",
        "\n",
        "if permutation_done and reddit_500:\n",
        "    !gsutil cp gs://masterthesisbert/parameter_optimization/permutation.csv  /content/permutation.csv\n",
        "    from numpy import loadtxt\n",
        "    p = loadtxt('permutation.csv', delimiter=',')\n",
        "    data_train_x_permut, train_y_tensor_permut = data_train_x[p.astype(int)], train_y_tensor[p.astype(int)]\n",
        "\n",
        "\n",
        "elif(permutation_done==False and reddit_500==True):\n",
        "    \n",
        "    data_train_x_permut, train_y_tensor_permut, p = unison_shuffled_copies(data_train_x, train_y_tensor)\n",
        "\n",
        "    from numpy import savetxt\n",
        "    savetxt('permutation.csv', p, delimiter=',')\n",
        "\n",
        "    from numpy import loadtxt\n",
        "    p = loadtxt('permutation.csv', delimiter=',')\n",
        "    p\n",
        "    #!gsutil cp /content/permutation.csv gs://masterthesisbert/parameter_optimization/permutation.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnGZroqCswDZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "if ten_classes:\n",
        "    train_y_tensor = to_categorical(train_y_tensor)\n",
        "    train_y_tensor2 = to_categorical(train_y_tensor2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Novelty Detection"
      ],
      "metadata": {
        "id": "UDgDhzSjzxfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novelty_detection = True\n",
        "if novelty_detection:\n",
        "    #for the novelty detection the data is already permutated\n",
        "    !gsutil cp gs://masterthesisbert/anomaly_indices/BERT_Aladag_noisy1.txt /content/\n",
        "    #!gsutil cp gs://masterthesisbert/anomaly_indices/PsychBERT1_reddit_500_noisy1.txt /content/\n",
        "\n",
        "    my_file = open(\"/content/BERT_Aladag_noisy1.txt\", \"r\")\n",
        "    content = my_file.read()\n",
        "    normal_data = content.split(\"\\n\")\n",
        "    normal_data.pop()\n",
        "\n",
        "    normal_data = [int(i) for i in normal_data]\n",
        "    normal_data = sorted(normal_data)\n",
        "\n",
        "    if reddit_500:\n",
        "        data_train_x_permut = data_train_x_permut[normal_data]\n",
        "        train_y_tensor_permut = train_y_tensor_permut[normal_data]\n",
        "    elif aladag:\n",
        "        data_train_x = data_train_x[normal_data]\n",
        "        train_y_tensor = train_y_tensor[normal_data]\n"
      ],
      "metadata": {
        "id": "5WAQ3n_rzwnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_results(grid_result, model_name, result_to_file = True):\n",
        "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "    means = grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "    for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "    if result_to_file == True:\n",
        "        string_to_save = f\"Best: {grid_result.best_score_} using {grid_result.best_params_} \\n\" \n",
        "        for mean, stdev, param in zip(means, stds, params):\n",
        "            string_to_save = string_to_save + f\"{mean} ({stdev}) with: {param} \\n\" \n",
        "        text_results = open(model_name + \".txt\", \"w\")\n",
        "        n = text_results.write(string_to_save)\n",
        "        text_results.close()\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "tLu3qT5TZUXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxDu8ryEk0o2"
      },
      "source": [
        "#Simple Dense 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXP1tAW_bmIm"
      },
      "outputs": [],
      "source": [
        "# Fully Dense Network\n",
        "\n",
        "def test_dense(data_train_x=data_train_x, number_of_classes = 2, learn_rate=0.001, momentum = 0.2, dropout_rate=0.3):\n",
        "    number = 2\n",
        "    os.environ['PYTHONHASHSEED']=str(number)\n",
        "    #Set random seed for numpy, python and tensorflow\n",
        "    np.random.seed(number)\n",
        "    rn.seed(number)\n",
        "    tf.set_random_seed(number)\n",
        "    # Set the number of threads to 1 \n",
        "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "    K.set_session(sess)\n",
        "\n",
        "    dense = Sequential()\n",
        "    dense.add(Input(shape=(768,)))\n",
        "    dense.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    dense.add(Dense(384, activation='relu',  kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "    dense.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    dense.add(Dense(128, activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "    dense.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    dense.add(Dense(64, activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "    dense.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "    if number_of_classes==2:\n",
        "\n",
        "        dense.add(Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "        lossi = 'binary_crossentropy'\n",
        "\n",
        "    elif number_of_classes==10:\n",
        "        dense.add(Dense(10, activation='softmax', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "        lossi = \"categorical_crossentropy\"\n",
        "\n",
        "        \n",
        "    #opt = keras.optimizers.Adam(momentum=momentum)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learn_rate)\n",
        "\n",
        "    dense.compile(optimizer=optimizer,\n",
        "                  loss=lossi,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #mc = ModelCheckpoint(dense_path + \".h5\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "    dense.summary()\n",
        "\n",
        "    return dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBxXxY40U08c"
      },
      "outputs": [],
      "source": [
        "validation = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMC_ruWccWZW"
      },
      "outputs": [],
      "source": [
        "if gridsearch:\n",
        "    if (validation==False):\n",
        "        if reddit_500:\n",
        "        \n",
        "            model_autoencoder_LSTM = KerasClassifier(build_fn=test_dense, epochs=5, learn_rate=0.0001, dropout_rate=0.3)\n",
        "\n",
        "        if ten_classes:\n",
        "            model_autoencoder_LSTM = KerasClassifier(build_fn=test_dense, epochs=5, learn_rate=0.0001, dropout_rate=0.3,  number_of_classes = 10)\n",
        "\n",
        "\n",
        "\n",
        "        learn_rate = [0.0001, 0.00001]\n",
        "        dropout_rate = [0.4, 0.5, 0.55]\n",
        "        epochs= [100, 120]\n",
        "        params=dict(learn_rate = learn_rate,\n",
        "                    dropout_rate = dropout_rate,\n",
        "                    epochs=epochs\n",
        "\n",
        "        )\n",
        "        grid = GridSearchCV(estimator=model_autoencoder_LSTM, param_grid=params, cv=5)\n",
        "\n",
        "    if (validation==False):\n",
        "        #if dataset is reddit 500 use data_train_x_permut and train_y_tensor_permut otherwise use data_train_x\n",
        "        if reddit_500:\n",
        "\n",
        "            grid_result = grid.fit(data_train_x_permut, train_y_tensor_permut)\n",
        "\n",
        "        else:\n",
        "            grid_result = grid.fit(data_train_x, train_y_tensor)\n",
        "\n",
        "        model_name = \"PsychBERT_con_long_ten_classes_Dense\"\n",
        "        model_name_txt = model_name + \".txt\"\n",
        "        summarize_results(grid_result, model_name)\n",
        "\n",
        "        !gsutil cp /content/{model_name_txt} gs://masterthesisbert/parameter_optimization/{model_name_txt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg87cEeyH6Fr"
      },
      "outputs": [],
      "source": [
        "validation = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZaoc9efGSB"
      },
      "outputs": [],
      "source": [
        "if validation:\n",
        "    final_dense = test_dense(data_train_x_permut, learn_rate=0.0001, dropout_rate=0.4, number_of_classes = 2)\n",
        "    final_dense.fit(data_train_x_permut, train_y_tensor_permut,  epochs = 120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxCccmrT-jGX"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    \n",
        "    \n",
        "    \"\"\"Function to calculate accuracy\n",
        "    -> param y_true: list of true values\n",
        "    -> param y_pred: list of predicted values\n",
        "    -> return: accuracy score\n",
        "    \"\"\"\n",
        "       \n",
        "    # Intitializing variable to store count of correctly predicted classes\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        \n",
        "        if np.array_equal(yt, yp):\n",
        "            \n",
        "            correct_predictions += 1\n",
        "    \n",
        "    #returns accuracy\n",
        "    return correct_predictions / len(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiioguqlJpl7"
      },
      "outputs": [],
      "source": [
        "if validation:\n",
        "    predicted2 = final_dense.predict(data_train_x2)\n",
        "    if ten_classes: \n",
        "        b = np.zeros_like(predicted2)\n",
        "        b[np.arange(len(predicted2)), predicted2.argmax(1)] = 1\n",
        "        predicted2 = b\n",
        "    else:\n",
        "        predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "\n",
        "\n",
        "    model_text = \"\"\n",
        "\n",
        "    model_text = model_text + \"PsychBERT2 Dense trained on 10 classes validated on 10 classes\"\n",
        "    print(model_text)\n",
        "    #accuracy\n",
        "\n",
        "    model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor2, predicted2), 5)) \n",
        "    \n",
        "    print(\"Accuracy:\", round(accuracy_score(train_y_tensor2, predicted2), 5))\n",
        "\n",
        "    if ten_classes:\n",
        "        #precision\n",
        "        model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "        #recall\n",
        "        model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "\n",
        "        #F1Score\n",
        "        model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "        #precision\n",
        "        model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2), 5))\n",
        "        #recall\n",
        "        model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2), 5))\n",
        "\n",
        "        #F1Score\n",
        "        model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2), 5)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZyx7inamXnJ"
      },
      "source": [
        "#LSTM-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VLYZLp8nQ8E"
      },
      "outputs": [],
      "source": [
        "def create_lstm_cnn(data_train_x=data_train_x, training = True, number_of_classes=2, learn_rate=0.001, momentum = 0.2, dropout_rate=0.3):\n",
        "    number = 2\n",
        "    os.environ['PYTHONHASHSEED']=str(number)\n",
        "    #Set random seed for numpy, python and tensorflow\n",
        "    np.random.seed(number)\n",
        "    rn.seed(number)\n",
        "    tf.set_random_seed(number)\n",
        "    # Set the number of threads to 1 \n",
        "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "    K.set_session(sess)\n",
        "\n",
        "\n",
        "    lstm_cnn = Sequential() \n",
        "\n",
        "    lstm_cnn.add(Input(shape=(256,768)))\n",
        "\n",
        "    lstm_cnn.add(Dropout(0.5)) \n",
        "    lstm_cnn.add(LSTM(units=100, return_sequences=True)) \n",
        "    lstm_cnn.add(Conv1D(3, (8,), padding='same', activation='relu')) \n",
        "    lstm_cnn.add(MaxPooling1D(2))\n",
        "    lstm_cnn.add(Flatten())\n",
        "\n",
        "    if number_of_classes==2:\n",
        "\n",
        "        lstm_cnn.add(Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "        lossi = 'binary_crossentropy'\n",
        "\n",
        "    elif number_of_classes==10 and training == True:\n",
        "        lstm_cnn.add(Dense(10, activation='softmax', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "        lossi = \"categorical_crossentropy\"\n",
        "\n",
        "    elif number_of_classes==10 and training == False:\n",
        "      \n",
        "        lossi = \"categorical_crossentropy\"\n",
        "        lstm_cnn.add(Dense(10, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learn_rate)\n",
        "\n",
        "    lstm_cnn.compile(optimizer=optimizer,\n",
        "                  loss=lossi,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    lstm_cnn.summary()\n",
        "\n",
        "    return lstm_cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzJJfiF0njpS"
      },
      "outputs": [],
      "source": [
        "if gridsearch:\n",
        "\n",
        "    if ten_classes:\n",
        "        lstm_cnn = KerasClassifier(build_fn=create_lstm_cnn, epochs=2, learn_rate=0.0001, number_of_classes=10 )\n",
        "\n",
        "    else:\n",
        "\n",
        "        lstm_cnn = KerasClassifier(build_fn=create_lstm_cnn, epochs=2, learn_rate=0.0001)\n",
        "\n",
        "    learn_rate = [0.0001]\n",
        "\n",
        "    epochs= [15, 20]\n",
        "    params=dict(learn_rate = learn_rate,\n",
        "                epochs=epochs\n",
        "\n",
        "    )\n",
        "    grid = GridSearchCV(estimator=lstm_cnn, param_grid=params, cv=5)\n",
        "\n",
        "    if reddit_500:\n",
        "        \n",
        "        grid_result = grid.fit(data_train_x_permut, train_y_tensor_permut)\n",
        "\n",
        "    else:\n",
        "        grid_result = grid.fit(data_train_x, train_y_tensor)\n",
        "\n",
        "\n",
        "    model_name = \"PsychBERT1_10classes_LSTM-CNN_part2\"\n",
        "    model_name_txt = model_name + \".txt\"\n",
        "    summarize_results(grid_result, model_name)\n",
        "\n",
        "    !gsutil cp /content/{model_name_txt} gs://masterthesisbert/parameter_optimization/{model_name_txt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBVrAouPu0dT"
      },
      "outputs": [],
      "source": [
        "#depeonding on the expected functionality set the validation true or false \n",
        "validation=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8VhDHTJKkp4"
      },
      "outputs": [],
      "source": [
        "#if train-test split, then this true (if gridsearch = False )\n",
        "train_different_manner = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each time the training set is the 4/5 of the normal data or chosen size\n",
        "train_test_split = int(round(len(data_train_x)/5, 0)*4)\n",
        "\n",
        "# for example \n",
        "#train_test_split = 4000\n",
        "train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqbsogyQumRe",
        "outputId": "826cac27-9d59-43d9-fe10-e4f85c20370f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7560"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_different_manner:\n",
        "\n",
        "    learn_rate = [0.0001, 0.00001]\n",
        "\n",
        "    epochs= [5, 10]\n",
        "\n",
        "    for i in range(len(learn_rate)):\n",
        "\n",
        "        for j inn range(len(epochs)):\n",
        "          \n",
        "            final_lstm_cnn = create_lstm_cnn(data_train_x[:train_test_split], learn_rate=learn_rate[i], number_of_classes=2)\n",
        "            final_lstm_cnn.fit(data_train_x[:train_test_split], train_y_tensor[:train_test_split],  epochs = epochs[j])\n",
        "\n",
        "            predicted2 = final_lstm_cnn.predict(data_train_x[train_test_split:])\n",
        "            print(learn_rate[i], epochs[j])\n",
        "\n",
        "            if ten_classes: \n",
        "                b = np.zeros_like(predicted2)\n",
        "                b[np.arange(len(predicted2)), predicted2.argmax(1)] = 1\n",
        "                predicted2 = b\n",
        "            else:\n",
        "                predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "\n",
        "            if(i==0 and j==0):\n",
        "\n",
        "                model_text = \"\" + \"\\n\"\n",
        "            else:\n",
        "                model_text = model_text + \"\\n\"\n",
        "\n",
        "            #accuracy\n",
        "            model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "            print(\"Accuracy:\", round(accuracy_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "            if ten_classes:\n",
        "                  #precision\n",
        "                  model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"Precision:\", round(precision_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "                  #recall\n",
        "                  model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"Recall:\", round(recall_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "\n",
        "                  #F1Score\n",
        "                  model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"F1score:\", round(f1_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "\n",
        "\n",
        "            else:\n",
        "\n",
        "                  #precision\n",
        "                  model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"Precision:\", round(precision_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "                  #recall\n",
        "                  model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"Recall:\", round(recall_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "\n",
        "                  #F1Score\n",
        "                  model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"F1score:\", round(f1_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "\n",
        "    print(model_text)\n",
        "\n",
        "    name = \"Noise_Results_PsychBERT1_LSTM-CNN_Aladag_notcross.txt\"\n",
        "\n",
        "    with open(name, \"w\") as text_file:\n",
        "            text_file.write(model_text)\n",
        "\n",
        "    !gsutil cp /content/{name} gs://masterthesisbert/parameter_optimization"
      ],
      "metadata": {
        "id": "Pt_FEA0_ueVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFtmIeV_SbGa"
      },
      "outputs": [],
      "source": [
        "final_lstm_cnn = create_lstm_cnn(data_train_x, learn_rate=0.0001, number_of_classes=2)\n",
        "final_lstm_cnn.fit(data_train_x, train_y_tensor,  epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwSWLgNMjdz8"
      },
      "outputs": [],
      "source": [
        "predicted2 = final_lstm_cnn.predict(data_train_x2)\n",
        "\n",
        "if ten_classes: \n",
        "        b = np.zeros_like(predicted2)\n",
        "        b[np.arange(len(predicted2)), predicted2.argmax(1)] = 1\n",
        "        predicted2 = b\n",
        "else:\n",
        "    predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "\n",
        "model_text = \"\" + \"\\n\"\n",
        "\n",
        "model_text = model_text + \"PsychBERT2 LSTM-CNN trained on rsdd\"\n",
        "print(model_text)\n",
        "#accuracy\n",
        "model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor2, predicted2), 5)) \n",
        "print(\"Accuracy:\", round(accuracy_score(train_y_tensor2, predicted2), 5))\n",
        "if ten_classes:\n",
        "      #precision\n",
        "      model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "      print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "      #recall\n",
        "      model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "      print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "\n",
        "      #F1Score\n",
        "      model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "      print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "\n",
        "else:\n",
        "\n",
        "      #precision\n",
        "      model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "      print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2), 5))\n",
        "      #recall\n",
        "      model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "      print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2), 5))\n",
        "\n",
        "      #F1Score\n",
        "      model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "      print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2), 5)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXjYd7Mhk6rY"
      },
      "source": [
        "# LSTM-Attention-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFias_hxluKI"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\") \n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"zeros\")\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "\n",
        "    def call(self,x):\n",
        "\n",
        "        e = K.tanh(K.dot(x, self.W) +self.b) \n",
        "        a = K.softmax(e, axis=1) \n",
        "        output = x*a\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "\n",
        "        return K.sum(output, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t8VMwgMexEi"
      },
      "outputs": [],
      "source": [
        "def create_lstm_atten_cnn(data_train_x=data_train_x, number_of_classes=2, training = True, learn_rate=0.001, momentum = 0.2, dropout_rate=0.3):\n",
        "    number = 2\n",
        "    os.environ['PYTHONHASHSEED']=str(number)\n",
        "    #Set random seed for numpy, python and tensorflow\n",
        "    np.random.seed(number)\n",
        "    rn.seed(number)\n",
        "    tf.set_random_seed(number)\n",
        "    # Set the number of threads to 1 \n",
        "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "    K.set_session(sess)\n",
        "\n",
        "\n",
        "    lstm_atten_cnn = Sequential() \n",
        "\n",
        "    lstm_atten_cnn.add(Input(shape=(256,768)))\n",
        "\n",
        "    lstm_atten_cnn.add(Dropout(0.5)) \n",
        "    lstm_atten_cnn.add(LSTM(units=100, return_sequences=True)) \n",
        "    lstm_atten_cnn.add(Attention(return_sequences=True)) \n",
        "    lstm_atten_cnn.add(Conv1D(3, (8,), padding='same', activation='relu')) \n",
        "    lstm_atten_cnn.add(MaxPooling1D(2))\n",
        "    lstm_atten_cnn.add(Flatten()) \n",
        "\n",
        "    if number_of_classes==2:\n",
        "\n",
        "        lstm_atten_cnn.add(Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "        lossi = 'binary_crossentropy'\n",
        "\n",
        "    elif number_of_classes==10 and training == True:\n",
        "        lstm_atten_cnn.add(Dense(10, activation='softmax', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "        lossi = \"categorical_crossentropy\"\n",
        "\n",
        "    elif number_of_classes==10 and training == False:\n",
        "      \n",
        "        lossi = \"categorical_crossentropy\"\n",
        "        lstm_atten_cnn.add(Dense(10, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learn_rate)\n",
        "\n",
        "    lstm_atten_cnn.compile(optimizer=optimizer,\n",
        "                  loss=lossi,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #mc = ModelCheckpoint(dense_path + \".h5\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "    lstm_atten_cnn.summary()\n",
        "\n",
        "\n",
        "    return lstm_atten_cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOCDmLp3uUq-"
      },
      "outputs": [],
      "source": [
        "validation=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OZ1O_C9vtTWV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "if gridsearch:\n",
        "    if (validation==False):\n",
        "\n",
        "        if ten_classes:\n",
        "\n",
        "            lstm_atten_cnn = KerasClassifier(build_fn=create_lstm_atten_cnn, epochs=2, learn_rate=0.0001, number_of_classes=10)\n",
        "\n",
        "        else: \n",
        "            lstm_atten_cnn = KerasClassifier(build_fn=create_lstm_atten_cnn, epochs=2, learn_rate=0.0001)\n",
        "\n",
        "        learn_rate = [0.00001]\n",
        "\n",
        "        epochs= [20]\n",
        "        params=dict(learn_rate = learn_rate,\n",
        "                  epochs=epochs\n",
        "\n",
        "        )\n",
        "        grid = GridSearchCV(estimator=lstm_atten_cnn, param_grid=params, cv=5)\n",
        "\n",
        "        if reddit_500:\n",
        "    \n",
        "            grid_result = grid.fit(data_train_x_permut, train_y_tensor_permut)\n",
        "\n",
        "        else:\n",
        "            grid_result = grid.fit(data_train_x, train_y_tensor)\n",
        "\n",
        "        model_name = \"PsychBERT2_10classes_LSTM-attention-CNN_part2\"\n",
        "        model_name_txt = model_name + \".txt\"\n",
        "        summarize_results(grid_result, model_name)\n",
        "\n",
        "        !gsutil cp /content/{model_name_txt} gs://masterthesisbert/parameter_optimization/{model_name_txt} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp2FLDiM5aX9"
      },
      "outputs": [],
      "source": [
        "validation=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7EFiFXLJcLm"
      },
      "outputs": [],
      "source": [
        "train_different_manner = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYH7KGeE_1um"
      },
      "outputs": [],
      "source": [
        "if train_different_manner:\n",
        "\n",
        "    learn_rate = [0.0001, 0.00001]\n",
        "\n",
        "    epochs= [5, 10]\n",
        "\n",
        "    for i in range(len(learn_rate)):\n",
        "\n",
        "        for j inn range(len(epochs)):\n",
        "          \n",
        "            final_lstm_a_cnn = lstm_atten_cnn(data_train_x[:train_test_split], learn_rate=learn_rate[i], number_of_classes=2)\n",
        "            final_lstm_a_cnn.fit(data_train_x[:train_test_split], train_y_tensor[:train_test_split],  epochs = epochs[j])\n",
        "\n",
        "            predicted2 = final_lstm_a_cnn.predict(data_train_x[train_test_split:])\n",
        "            print(learn_rate[i], epochs[j])\n",
        "\n",
        "            if ten_classes: \n",
        "                b = np.zeros_like(predicted2)\n",
        "                b[np.arange(len(predicted2)), predicted2.argmax(1)] = 1\n",
        "                predicted2 = b\n",
        "            else:\n",
        "                predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "\n",
        "            if(i==0 and j==0):\n",
        "\n",
        "                model_text = \"\" + \"\\n\"\n",
        "            else:\n",
        "                model_text = model_text + \"\\n\"\n",
        "\n",
        "            #accuracy\n",
        "            model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "            print(\"Accuracy:\", round(accuracy_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "            if ten_classes:\n",
        "                  #precision\n",
        "                  model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"Precision:\", round(precision_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "                  #recall\n",
        "                  model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"Recall:\", round(recall_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "\n",
        "                  #F1Score\n",
        "                  model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"F1score:\", round(f1_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "\n",
        "\n",
        "            else:\n",
        "\n",
        "                  #precision\n",
        "                  model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"Precision:\", round(precision_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "                  #recall\n",
        "                  model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"Recall:\", round(recall_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "\n",
        "                  #F1Score\n",
        "                  model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"F1score:\", round(f1_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "\n",
        "    print(model_text)\n",
        "\n",
        "    name = \"Noise_Results_PsychBERT1_LSTM-Attention-CNN_Aladag_notcross.txt\"\n",
        "\n",
        "    with open(name, \"w\") as text_file:\n",
        "            text_file.write(model_text)\n",
        "\n",
        "    !gsutil cp /content/{name} gs://masterthesisbert/parameter_optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWLCnVXYjKcy"
      },
      "outputs": [],
      "source": [
        "#if (validation==True):\n",
        "final_lstm_attention_cnn = create_lstm_atten_cnn(data_train_x, learn_rate=0.0001, number_of_classes=2)\n",
        "final_lstm_attention_cnn.fit(data_train_x, train_y_tensor,  epochs = 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTc3Ypt6Rfaz"
      },
      "outputs": [],
      "source": [
        "validation=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS6_MnR2kuVO"
      },
      "outputs": [],
      "source": [
        "if validation:\n",
        "    predicted2 = final_lstm_attention_cnn.predict(data_train_x2)\n",
        "\n",
        "    if ten_classes: \n",
        "        b = np.zeros_like(predicted2)\n",
        "        b[np.arange(len(predicted2)), predicted2.argmax(1)] = 1\n",
        "        predicted2 = b\n",
        "    else:\n",
        "        predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "\n",
        "    model_text = \"model_text\" + \"\\n\"\n",
        "\n",
        "    model_text = model_text + \"PsychBERT2 LSTM-Atten-CNN trained on rsdd \"\n",
        "    print(model_text)\n",
        "    #accuracy\n",
        "    model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor2, predicted2), 5)) \n",
        "    print(\"Accuracy:\", round(accuracy_score(train_y_tensor2, predicted2), 5))\n",
        "    if ten_classes:\n",
        "        #precision\n",
        "        model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "        #recall\n",
        "        model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "\n",
        "        #F1Score\n",
        "        model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "        #precision\n",
        "        model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2), 5))\n",
        "        #recall\n",
        "        model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2), 5))\n",
        "\n",
        "        #F1Score\n",
        "        model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2), 5)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DsydayFDznF"
      },
      "source": [
        "#Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9b7aWhTDysB"
      },
      "outputs": [],
      "source": [
        "def create_bilstm(data_train_x=data_train_x, number_of_classes=2, learn_rate=0.0001, training = True, momentum = 0.2, dropout_rate=0.3):\n",
        "    # Bi-LSTM\n",
        "    bilstm = Sequential()\n",
        "\n",
        "    bilstm_path = \"bilstm\"\n",
        "\n",
        "    pool_size = 2\n",
        "\n",
        "    bilstm.add(Input(shape=(256,768)))\n",
        "    bilstm.add(Bidirectional(LSTM(20, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.2)))\n",
        "    bilstm.add(MaxPooling1D(pool_size = pool_size))\n",
        "    bilstm.add(Flatten())\n",
        "    bilstm.add(Dropout(0.5)) \n",
        "\n",
        "    if number_of_classes==2:\n",
        "\n",
        "        bilstm.add(Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "        lossi = 'binary_crossentropy'\n",
        "\n",
        "    elif number_of_classes==10 and training == True:\n",
        " \n",
        "        bilstm.add(Dense(10, activation='softmax', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "        lossi = \"categorical_crossentropy\"\n",
        "\n",
        "    elif number_of_classes==10 and training == False:\n",
        "      \n",
        "        lossi = \"categorical_crossentropy\"\n",
        "        bilstm.add(Dense(10, activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learn_rate)\n",
        "\n",
        "    bilstm.compile(optimizer=optimizer,\n",
        "                  loss=lossi,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    bilstm.summary()\n",
        "    return bilstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_zelq6xhxVB"
      },
      "outputs": [],
      "source": [
        "if gridsearch:\n",
        "    if (validation==False):\n",
        "        if ten_classes:\n",
        "\n",
        "            bilstm = KerasClassifier(build_fn=create_bilstm, epochs=2, learn_rate=0.0001, number_of_classes=10)\n",
        "\n",
        "        else: \n",
        "            bilstm = KerasClassifier(build_fn=create_bilstm, epochs=2, learn_rate=0.0001)\n",
        "\n",
        "\n",
        "        learn_rate = [0.00001]\n",
        "\n",
        "        epochs= [20]\n",
        "        params=dict(learn_rate = learn_rate,\n",
        "                    epochs=epochs)\n",
        "        \n",
        "        grid = GridSearchCV(estimator=bilstm, param_grid=params, cv=5)\n",
        "\n",
        "        if reddit_500:\n",
        "    \n",
        "            grid_result = grid.fit(data_train_x_permut, train_y_tensor_permut)\n",
        "\n",
        "        else:\n",
        "            grid_result = grid.fit(data_train_x, train_y_tensor)\n",
        "\n",
        "        model_name = \"PsychBERT2_10classes_BiLSTM_part2\"\n",
        "        model_name_txt = model_name + \".txt\"\n",
        "        summarize_results(grid_result, model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psJR21tL0Etp"
      },
      "outputs": [],
      "source": [
        "if (validation==False):\n",
        "    !gsutil cp /content/{model_name_txt} gs://masterthesisbert/parameter_optimization/{model_name_txt}  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jcnm1h-CyTO"
      },
      "outputs": [],
      "source": [
        "validation = True  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzwVD7_sZVlo"
      },
      "outputs": [],
      "source": [
        "train_different_manner = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each time the training set is the 4/5 of the normal data\n",
        "train_test_split = int(round(len(data_train_x)/5, 0)*4)\n",
        "train_test_split"
      ],
      "metadata": {
        "id": "4qmO-rQnA0Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if reddit_500:\n",
        "    data_train_x = data_train_x_permut\n",
        "    train_y_tensor = train_y_tensor_permut"
      ],
      "metadata": {
        "id": "pvHg5nS3Eh5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW5iuTgWdbK9"
      },
      "outputs": [],
      "source": [
        "if train_different_manner:\n",
        "\n",
        "    learn_rate = [0.0001, 0.00001]\n",
        "\n",
        "    epochs= [5, 10]\n",
        "\n",
        "    for i in range(len(learn_rate)):\n",
        "\n",
        "        for j in range(len(epochs)):\n",
        "          \n",
        "            final_bi_lstm = create_bilstm(data_train_x[:train_test_split], learn_rate=learn_rate[i], number_of_classes=2)\n",
        "            final_bi_lstm.fit(data_train_x[:train_test_split], train_y_tensor[:train_test_split],  epochs = epochs[j])\n",
        "\n",
        "            predicted2 = final_bi_lstm.predict(data_train_x[train_test_split:])\n",
        "            print(learn_rate[i], epochs[j])\n",
        "\n",
        "            if ten_classes: \n",
        "                b = np.zeros_like(predicted2)\n",
        "                b[np.arange(len(predicted2)), predicted2.argmax(1)] = 1\n",
        "                predicted2 = b\n",
        "            else:\n",
        "                predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "\n",
        "            if(i==0 and j==0):\n",
        "\n",
        "                model_text = \"\" + \"\\n\"\n",
        "            else:\n",
        "                model_text = model_text + \"\\n\"\n",
        "\n",
        "            #accuracy\n",
        "            model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "            print(\"Accuracy:\", round(accuracy_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "            if ten_classes:\n",
        "                  #precision\n",
        "                  model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"Precision:\", round(precision_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "                  #recall\n",
        "                  model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"Recall:\", round(recall_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "\n",
        "                  #F1Score\n",
        "                  model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5)) \n",
        "\n",
        "                  print(\"F1score:\", round(f1_score(train_y_tensor[train_test_split:], predicted2, average='weighted'), 5))\n",
        "\n",
        "\n",
        "            else:\n",
        "\n",
        "                  #precision\n",
        "                  model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"Precision:\", round(precision_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "                  #recall\n",
        "                  model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"Recall:\", round(recall_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "\n",
        "                  #F1Score\n",
        "                  model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "                  print(\"F1score:\", round(f1_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "\n",
        "    print(model_text)\n",
        "\n",
        "    name = \"Noise_Results_BERT_BiLSTM_Aladag_notcross.txt\"\n",
        "\n",
        "    with open(name, \"w\") as text_file:\n",
        "            text_file.write(model_text)\n",
        "\n",
        "    !gsutil cp /content/{name} gs://masterthesisbert/parameter_optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if validation:\n",
        "    final_bi_lstm = create_bilstm(data_train_x, learn_rate=0.0001, number_of_classes=2)\n",
        "    final_bi_lstm.fit(data_train_x, train_y_tensor,  epochs = 10)"
      ],
      "metadata": {
        "id": "sgaTeITswZSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bOqJjiwmxKD"
      },
      "outputs": [],
      "source": [
        "if validation:   \n",
        "    predicted2 = final_bi_lstm.predict(data_train_x2)\n",
        "\n",
        "    if ten_classes: \n",
        "        b = np.zeros_like(predicted2)\n",
        "        b[np.arange(len(predicted2)), predicted2.argmax(1)] = 1\n",
        "        predicted2 = b\n",
        "    else:\n",
        "        predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "\n",
        "    model_text = \"model_text\" + \"\\n\"\n",
        "\n",
        "    model_text = model_text + \"BERT Bi-LSTM trained rsdd\"\n",
        "    print(model_text)\n",
        "    #accuracy\n",
        "    model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor2, predicted2), 5)) \n",
        "    print(\"Accuracy:\", round(accuracy_score(train_y_tensor2, predicted2), 5))\n",
        "    if ten_classes:\n",
        "        #precision\n",
        "        model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "        #recall\n",
        "        model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "\n",
        "        #F1Score\n",
        "        model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2, average='weighted'), 5)) \n",
        "\n",
        "        print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2, average='weighted'), 5))\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "        #precision\n",
        "        model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2), 5))\n",
        "        #recall\n",
        "        model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2), 5))\n",
        "\n",
        "        #F1Score\n",
        "        model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "        print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2), 5)) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "FirstClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}