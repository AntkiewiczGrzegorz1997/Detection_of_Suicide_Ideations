{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNCBkC2mSxbr"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install \"tensorflow_hub>=0.6.0\"\n",
        "!pip3 install tensorflow_text==1.15\n",
        "!pip install transformers\n",
        "!pip install numpy==1.19.5\n",
        "!pip install 'h5py==2.10.0'\n",
        "!pip install natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6yKx0hTTI-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers as ppb\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import natsort\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, MaxPooling1D, Dropout, Conv1D, Input, LSTM, SpatialDropout1D, Bidirectional, Conv1D, MaxPooling2D, Layer, concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i0WTGYMTfEG",
        "outputId": "72bae4a1-f9f3-4f5c-e21e-172c42f27cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.19.5\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZPgo-iiTiAQ",
        "outputId": "59ff37bd-8543-4e22-f7cb-c50097c302f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.10.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "print(h5py.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHKdTsZK5Hnq"
      },
      "source": [
        "#IF the numpy version not equal to 1.19.5 and h5py version not equal to 2.10.0 restart the runtime and start again from cell 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQXj6VqETk8A"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!curl https://sdk.cloud.google.com | bash\n",
        "\n",
        "!gcloud init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s52AH13FGsz9"
      },
      "outputs": [],
      "source": [
        "validation = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R87U1jCgUccx"
      },
      "outputs": [],
      "source": [
        "!mkdir training_directory\n",
        "!mkdir validation_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo_2RbBuUcfC"
      },
      "outputs": [],
      "source": [
        "if validation:\n",
        "    \n",
        "    #!gsutil -m cp gs://masterthesisbert/embeddings_labeled/PsychBERT_con_long/MyBERT_wide_full-training-features_2D_labeled_reddit_500* /content/validation_directory/\n",
        "    #!gsutil -m cp gs://masterthesisbert/embeddings_right/PsychBERT_con_long/MyBERT_wide_full-training-features_2D_labeled_reddit_500* /content/training_directory/\n",
        "    !gsutil -m cp gs://masterthesisbert/embeddings_labeled/BERT/MyBERT_wide_full-training-features_3D_labeled_reddit_500_val* /content/validation_directory/\n",
        "    !gsutil -m cp gs://masterthesisbert/embeddings_right/BERT/MyBERT_wide_full-training-features_3D_labeled_reddit_500_tr* /content/training_directory/\n",
        "    \n",
        "else:\n",
        "    \n",
        "    #!gsutil -m cp gs://masterthesisbert/embeddings_right/PsychBERT_con_long/MyBERT_wide_full-training-features_3D_labeled_Alada* /content/training_directory/\n",
        "    #!gsutil -m cp gs://masterthesisbert/embeddings_right/PsychBERT_con_long/MyBERT_wide_full-training-features_2D_labeled_Alada* /content/training_directory/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C84SNKjSUchT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if validation:\n",
        "    arr = os.listdir(\"/content/training_directory\")\n",
        "    arr = natsort.natsorted(arr)\n",
        "    print(arr)\n",
        "    arrval = os.listdir(\"/content/validation_directory\")\n",
        "    arrval = natsort.natsorted(arrval)\n",
        "    print(arrval)\n",
        "\n",
        "\n",
        "else:\n",
        "    arr = os.listdir(\"/content/training_directory\")\n",
        "    arr = natsort.natsorted(arr)\n",
        "    print(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS_BT925EsTQ"
      },
      "outputs": [],
      "source": [
        "# load numpy array from csv file\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "\n",
        "third_dimension = 768\n",
        "\n",
        "arrays = []\n",
        "arrays2 = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aladag = False\n",
        "reddit_500 = True\n",
        "\n",
        "if aladag:\n",
        "    length = 10\n",
        "    length_1 = 1\n",
        "elif reddit_500:\n",
        "    length = 5\n",
        "    length_1 = 2"
      ],
      "metadata": {
        "id": "LDNHLBOoq1Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeGu8uu-Uclm"
      },
      "outputs": [],
      "source": [
        "if(validation==True):\n",
        "    for i in range(len(arr[0:length])):\n",
        "        print(arr[i])\n",
        "        loaded_arr = np.loadtxt('/content/training_directory'+\"/\"+arr[i])\n",
        "        arrays.append(loaded_arr)\n",
        "        print(i)\n",
        "    \n",
        "    for i in range(length_1):\n",
        "      print(arrval[i])\n",
        "      loaded_arr2 = np.loadtxt('/content/validation_directory'+\"/\"+arrval[i])\n",
        "      arrays2.append(loaded_arr2)\n",
        "      print(i)\n",
        "    \n",
        "else:\n",
        "    \n",
        "    for i in range(len(arr[0:length])):\n",
        "       \n",
        "        loaded_arr = np.loadtxt('/content/training_directory'+\"/\"+arr[i])\n",
        "        arrays.append(loaded_arr)\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP_s38fGUcnn"
      },
      "outputs": [],
      "source": [
        "threeD = True\n",
        "if threeD:\n",
        "    \n",
        "    concatenated_array = np.array([]).reshape(0, arrays[2].shape[1])\n",
        "\n",
        "    for array in arrays:\n",
        "\n",
        "        concatenated_array = np.concatenate([concatenated_array, array], axis=0)\n",
        "\n",
        "    del arrays\n",
        "\n",
        "    load_original_arr = concatenated_array.reshape(\n",
        "        concatenated_array.shape[0], concatenated_array.shape[1] // third_dimension, third_dimension)\n",
        "    del concatenated_array\n",
        "    data_train_x = load_original_arr\n",
        "\n",
        "    \n",
        "    concatenated_array2 = np.array([]).reshape(0, arrays2[0].shape[1])\n",
        "\n",
        "    for array in arrays2:\n",
        "\n",
        "        concatenated_array2 = np.concatenate([concatenated_array2, array], axis=0)\n",
        "\n",
        "    del arrays2\n",
        "\n",
        "    load_original_arr2 = concatenated_array2.reshape(\n",
        "        concatenated_array2.shape[0], concatenated_array2.shape[1] // third_dimension, third_dimension)\n",
        "    del concatenated_array2\n",
        "    data_train_x2 = load_original_arr2\n",
        "    \n",
        "    \n",
        "else:\n",
        "\n",
        "    if(validation==False):\n",
        "\n",
        "        concatenated_array = np.array([]).reshape(0, 768)\n",
        "        # Create an array to return to\n",
        "\n",
        "        for array in arrays:\n",
        "\n",
        "            concatenated_array = np.concatenate([concatenated_array, array], axis=0)\n",
        "        data_train_x = concatenated_array\n",
        "    else: \n",
        "        concatenated_array = np.array([]).reshape(0, 768)\n",
        "        # Create an array to return to\n",
        "\n",
        "        for array in arrays:\n",
        "\n",
        "            concatenated_array = np.concatenate([concatenated_array, array], axis=0)\n",
        "        data_train_x = concatenated_array\n",
        "\n",
        "        concatenated_array2 = np.array([]).reshape(0, 768)\n",
        "        # Create an array to return to\n",
        "\n",
        "        for array in arrays2:\n",
        "\n",
        "            concatenated_array2 = np.concatenate([concatenated_array2, array], axis=0)\n",
        "        data_train_x2 = concatenated_array2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9DONEEGUcr5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "if aladag:  \n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_sample_preprocessed.csv  /content/Aladag_sample_preprocessed.csv\n",
        "    \n",
        "    df_name = \"/content/Aladag_sample_preprocessed.csv\"\n",
        "    df = pd.read_csv(df_name)\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_labeled_preprocessed.csv  /content/Aladag_labeled_preprocessed.csv\n",
        "\n",
        "    df_name2 = \"/content/Aladag_labeled_preprocessed.csv\"\n",
        "    df2 = pd.read_csv(df_name2)\n",
        "    df2\n",
        "\n",
        "    train_y_tensor = torch.tensor(df[0:10000].binary_annotation.values)\n",
        "    train_y_tensor2 = torch.tensor(df2[0:10000].binary_annotation.values)\n",
        "\n",
        "   \n",
        "    !gsutil cp gs://masterthesisbert/Aladag_labeled_preprocessed_features.csv  /content/Aladag_labeled_preprocessed_features.csv\n",
        "    !gsutil cp gs://masterthesisbert/Aladag_sample_preprocessed_features.csv  /content/Aladag_sample_preprocessed_features.csv\n",
        "\n",
        "    df_name2_features = \"Aladag_labeled_preprocessed_features.csv\"\n",
        "    df_name_features = \"Aladag_sample_preprocessed_features.csv\"\n",
        "\n",
        "    df_features = pd.read_csv(df_name_features)\n",
        "\n",
        "    df2_features = pd.read_csv(df_name2_features)\n",
        "\n",
        "\n",
        "    a = [\"anxiety\", \"depression\", \"control\", \"ADHD\", \"bipolar disorder\", \"autism\", \"PTSD\", \"OCD\", \"schizophrenia\", \"eating disorder\"]\n",
        "    diseases = sorted(map(lambda x: x.lower(), a))\n",
        "    diseases\n",
        "\n",
        "    train_x_tensor_diseases = torch.tensor(df_features[0:10000][diseases].values)\n",
        "    train_x_tensor2_diseases = torch.tensor(df2_features[0:10000][diseases].values)\n",
        "\n",
        "    sentiment = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "    train_x_tensor_sentiment = torch.tensor(df_features[0:10000][sentiment].values)\n",
        "    train_x_tensor2_sentiment = torch.tensor(df2_features[0:10000][sentiment].values)\n",
        "\n",
        "    train_x_tensor_pronouns = torch.tensor(df_features[0:10000][[\"first\",\"third\"]].values)\n",
        "    train_x_tensor2_pronouns = torch.tensor(df2_features[0:10000][[\"first\",\"third\"]].values)\n",
        "\n",
        "    train_x_tensor_emotions= torch.tensor(df_features[0:10000][[\"anger\", \"fear\", \"joy\", \"sadness\"]].values)\n",
        "    train_x_tensor2_emotions = torch.tensor(df2_features[0:10000][[\"anger\", \"fear\", \"joy\", \"sadness\"]].values)\n",
        "\n",
        "    # all together \n",
        "    all_features = diseases + [\"negative\", \"neutral\", \"positive\"] + [\"first\",\"third\"] + [\"anger\", \"fear\", \"joy\", \"sadness\"]\n",
        "\n",
        "    train_x_tensor_features_full = torch.tensor(df_features[0:10000][all_features].values)\n",
        "    train_x_tensor2_features_full = torch.tensor(df2_features[0:10000][all_features].values)\n",
        "\n",
        "elif reddit_500:\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_val.csv  /content/reddit_500_final_val.csv\n",
        "\n",
        "    df_name2 = \"/content/reddit_500_final_val.csv\"\n",
        "    df2 = pd.read_csv(df_name2)\n",
        "    df2\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_train.csv  /content/reddit_500_final_train.csv\n",
        "\n",
        "    df_name = \"/content/reddit_500_final_train.csv\"\n",
        "    df = pd.read_csv(df_name)\n",
        "    df\n",
        "\n",
        "    train_y_tensor = torch.tensor(df[0:10000].Label.values)\n",
        "    train_y_tensor2 = torch.tensor(df2[0:10000].Label.values)\n",
        "\n",
        "\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_val_features.csv  /content/reddit_500_final_val_features.csv\n",
        "    !gsutil cp gs://masterthesisbert/reddit_500_final_train_features.csv /content/reddit_500_final_train_features.csv\n",
        "\n",
        "    df_name2_features = \"reddit_500_final_val_features.csv\"\n",
        "    df_name_features = \"reddit_500_final_train_features.csv\"\n",
        "\n",
        "    df_features = pd.read_csv(df_name_features)\n",
        "\n",
        "    df2_features = pd.read_csv(df_name2_features)\n",
        "\n",
        "\n",
        "    a = [\"anxiety\", \"depression\", \"control\", \"ADHD\", \"bipolar disorder\", \"autism\", \"PTSD\", \"OCD\", \"schizophrenia\", \"eating disorder\"]\n",
        "    diseases = sorted(map(lambda x: x.lower(), a))\n",
        "    diseases\n",
        "\n",
        "    train_x_tensor_diseases = torch.tensor(df_features[0:10000][diseases].values)\n",
        "    train_x_tensor2_diseases = torch.tensor(df2_features[0:10000][diseases].values)\n",
        "\n",
        "    sentiment = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "    train_x_tensor_sentiment = torch.tensor(df_features[0:10000][sentiment].values)\n",
        "    train_x_tensor2_sentiment = torch.tensor(df2_features[0:10000][sentiment].values)\n",
        "\n",
        "    train_x_tensor_pronouns = torch.tensor(df_features[0:10000][[\"first\",\"third\"]].values)\n",
        "    train_x_tensor2_pronouns = torch.tensor(df2_features[0:10000][[\"first\",\"third\"]].values)\n",
        "\n",
        "    train_x_tensor_emotions= torch.tensor(df_features[0:10000][[\"anger\", \"fear\", \"joy\", \"sadness\"]].values)\n",
        "    train_x_tensor2_emotions = torch.tensor(df2_features[0:10000][[\"anger\", \"fear\", \"joy\", \"sadness\"]].values)\n",
        "\n",
        "    # all together \n",
        "    all_features = diseases + [\"negative\", \"neutral\", \"positive\"] + [\"first\",\"third\"] + [\"anger\", \"fear\", \"joy\", \"sadness\"]\n",
        "\n",
        "    train_x_tensor_features_full = torch.tensor(df_features[0:10000][all_features].values)\n",
        "    train_x_tensor2_features_full = torch.tensor(df2_features[0:10000][all_features].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUd4wNY4bUpj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.layers import Input, Dense,RepeatVector, TimeDistributed, Dense, Dropout, LSTM, Bidirectional\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCMawPI9biwI",
        "outputId": "0472febd-9682-4647-fa83-e10ee3cd1729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.19.5\n"
          ]
        }
      ],
      "source": [
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "991CzKDNcg3T"
      },
      "outputs": [],
      "source": [
        "# Conctruct autoencoder for training test to find anomalous days in the data\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "import random as rn \n",
        "from tensorflow.compat.v1.keras import backend as K\n",
        "number = 2\n",
        "os.environ['PYTHONHASHSEED']=str(number)\n",
        "#Set random seed for numpy, python and tensorflow\n",
        "np.random.seed(number)\n",
        "rn.seed(number)\n",
        "tf.set_random_seed(number)\n",
        "# Set the number of threads to 1 \n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "# Its an example of an autoencoder. I might use it later. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afpjlaSmyd7L"
      },
      "outputs": [],
      "source": [
        "def unison_shuffled_copies(a, b, just_indices=False):\n",
        "        assert len(a) == len(b)\n",
        "        p = numpy.random.permutation(len(a))\n",
        "        return a[p], b[p], p "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL_rZRtuvzAh"
      },
      "outputs": [],
      "source": [
        "permutation_done = True\n",
        "\n",
        "if permutation_done and reddit_500:\n",
        "    !gsutil cp gs://masterthesisbert/parameter_optimization/permutation.csv  /content/permutation.csv\n",
        "    from numpy import loadtxt\n",
        "    p = loadtxt('permutation.csv', delimiter=',')\n",
        "    data_train_x_permut, train_y_tensor_permut = data_train_x[p.astype(int)], train_y_tensor[p.astype(int)]\n",
        "\n",
        "elif(permutation_done==False and reddit_500==True):\n",
        "    \n",
        "    data_train_x_permut, train_y_tensor_permut, p = unison_shuffled_copies(data_train_x, train_y_tensor)\n",
        "\n",
        "    from numpy import savetxt\n",
        "    savetxt('permutation.csv', p, delimiter=',')\n",
        "\n",
        "    from numpy import loadtxt\n",
        "    p = loadtxt('permutation.csv', delimiter=',')\n",
        "    p\n",
        "    #!gsutil cp /content/permutation.csv gs://masterthesisbert/parameter_optimization/permutation.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARnSNpzD2tEI"
      },
      "outputs": [],
      "source": [
        "noise_detection = True\n",
        "if noise_detection:\n",
        "    #for the novelty detection the data is already permutated\n",
        "    !gsutil cp gs://masterthesisbert/anomaly_indices/BERT_reddit_500_noisy1.txt /content/\n",
        "    #!gsutil cp gs://masterthesisbert/anomaly_indices/PsychBERT1_reddit_500_noisy1.txt /content/\n",
        "\n",
        "    my_file = open(\"/content/BERT_reddit_500_noisy1.txt\", \"r\")\n",
        "    content = my_file.read()\n",
        "    normal_data = content.split(\"\\n\")\n",
        "    normal_data.pop()\n",
        "\n",
        "    normal_data = [int(i) for i in normal_data]\n",
        "    normal_data = sorted(normal_data)\n",
        "\n",
        "    if reddit_500:\n",
        "        data_train_x_permut = data_train_x_permut[normal_data]\n",
        "        train_y_tensor_permut = train_y_tensor_permut[normal_data]\n",
        "    elif aladag:\n",
        "        data_train_x = data_train_x[normal_data]\n",
        "        train_y_tensor = train_y_tensor[normal_data]\n",
        "    \n",
        "    train_x_tensor_diseases =  train_x_tensor_diseases[normal_data]\n",
        "\n",
        "    sentiment = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "    train_x_tensor_sentiment = train_x_tensor_sentiment[normal_data]\n",
        "\n",
        "    train_x_tensor_pronouns = train_x_tensor_pronouns[normal_data]\n",
        "\n",
        "    train_x_tensor_emotions= train_x_tensor_emotions[normal_data]\n",
        "\n",
        "    # all together \n",
        "    all_features = diseases + [\"negative\", \"neutral\", \"positive\"] + [\"first\",\"third\"] + [\"anger\", \"fear\", \"joy\", \"sadness\"]\n",
        "\n",
        "    train_x_tensor_features_full = train_x_tensor_features_full[normal_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnGZroqCswDZ"
      },
      "outputs": [],
      "source": [
        "data_train_x = data_train_x_permut\n",
        "train_y_tensor = train_y_tensor_permut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XozPK1Rbd3Ek"
      },
      "outputs": [],
      "source": [
        "def create_bilstm(data_train_x=data_train_x, number_of_classes=2, learn_rate=0.0001, training = True, momentum = 0.2, dropout_rate=0.3):\n",
        "    # Bi-LSTM\n",
        "    bilstm = Sequential()\n",
        "\n",
        "    pool_size = 2\n",
        "\n",
        "    bilstm.add(Input(shape=(256,768)))\n",
        "    bilstm.add(Bidirectional(LSTM(20, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.2)))\n",
        "    bilstm.add(MaxPooling1D(pool_size = pool_size))\n",
        "    bilstm.add(Flatten())\n",
        "    bilstm.add(Dropout(0.5)) \n",
        "\n",
        "    return bilstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1hvPFGTkWFr"
      },
      "outputs": [],
      "source": [
        "def create_lstm_cnn(data_train_x=data_train_x, training = True, number_of_classes=2, learn_rate=0.001, momentum = 0.2, dropout_rate=0.3):\n",
        "\n",
        "    lstm_cnn = Sequential() \n",
        "\n",
        "    lstm_cnn.add(Input(shape=(256,768)))\n",
        "\n",
        "    lstm_cnn.add(Dropout(0.5)) \n",
        "    lstm_cnn.add(LSTM(units=100, return_sequences=True)) \n",
        "    lstm_cnn.add(Conv1D(3, (8,), padding='same', activation='relu')) \n",
        "    lstm_cnn.add(MaxPooling1D(2))\n",
        "    lstm_cnn.add(Flatten())\n",
        "\n",
        "    \n",
        "    return lstm_cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kz7H8nsnOo5"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\") \n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"zeros\")\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "\n",
        "    def call(self,x):\n",
        "\n",
        "        e = K.tanh(K.dot(x, self.W) +self.b) \n",
        "        a = K.softmax(e, axis=1) \n",
        "        output = x*a\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "def create_lstm_atten_cnn(data_train_x=data_train_x, number_of_classes=2, training = True, learn_rate=0.001, momentum = 0.2, dropout_rate=0.3):\n",
        "\n",
        "\n",
        "    lstm_atten_cnn = Sequential() \n",
        "\n",
        "    lstm_atten_cnn.add(Input(shape=(256,768)))\n",
        "\n",
        "    lstm_atten_cnn.add(Dropout(0.5)) \n",
        "    lstm_atten_cnn.add(LSTM(units=100, return_sequences=True)) \n",
        "    lstm_atten_cnn.add(Attention(return_sequences=True)) \n",
        "    lstm_atten_cnn.add(Conv1D(3, (8,), padding='same', activation='relu')) \n",
        "    lstm_atten_cnn.add(MaxPooling1D(2))\n",
        "    lstm_atten_cnn.add(Flatten()) \n",
        "\n",
        "    return lstm_atten_cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXP1tAW_bmIm"
      },
      "outputs": [],
      "source": [
        "# Fully Dense Network\n",
        "\n",
        "def create_dense(data_train_x=data_train_x, number_of_classes = 2, learn_rate=0.0001, momentum = 0.2, dropout_rate=0.3):\n",
        "\n",
        "    dense = Sequential()\n",
        "\n",
        "    dense.add(Input(shape=(768,)))\n",
        "\n",
        "    dense.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    dense.add(Dense(384, activation='relu',  kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "    dense.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    dense.add(Dense(128, activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "    dense.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    return dense\n",
        "\n",
        "def create_dense_num(data_train_x=train_x_tensor_diseases, number_of_classes = 2, learn_rate=0.0001, momentum = 0.2, dropout_rate=0.3):\n",
        "\n",
        "    dense_num = Sequential()\n",
        "\n",
        "\n",
        "    dense_num.add(Input(shape=(data_train_x.shape[1],)))\n",
        "\n",
        "    dense_num.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    dense_num.add(Dense(5, activation='relu',  kernel_initializer=keras.initializers.glorot_uniform(seed=number)))\n",
        "\n",
        "    dense_num.add(keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "    \n",
        "    return dense_num\n",
        "\n",
        "def concatenated_dense(data_train_x, train_x_tensor_diseases, learn_rate, dropout_rate=0.5, dense_la = True, bi = False, lstm_cnn_la = False, lstm_a_cnn_la=False):\n",
        "\n",
        "\n",
        "    dense_num = create_dense_num(data_train_x=train_x_tensor_diseases, learn_rate =learn_rate, dropout_rate=0.4)\n",
        "\n",
        "    # concatenate outputs\n",
        "\n",
        "    if dense_la:  \n",
        "        \n",
        "        dense = create_dense(data_train_x=data_train_x, learn_rate=learn_rate, dropout_rate=0.4)\n",
        "\n",
        "\n",
        "        combinedInput = concatenate([dense.output, dense_num.output])\n",
        "\n",
        "    elif bi:\n",
        "\n",
        "        bilstm = create_bilstm(data_train_x=data_train_x, learn_rate=learn_rate, dropout_rate=0.4)\n",
        "\n",
        "        combinedInput = concatenate([bilstm.output, dense_num.output])\n",
        "\n",
        "    elif lstm_cnn_la:\n",
        "\n",
        "        lstm_cnn = create_lstm_cnn(data_train_x=data_train_x, learn_rate=learn_rate, dropout_rate=0.4)\n",
        "\n",
        "        combinedInput = concatenate([lstm_cnn.output, dense_num.output])\n",
        "\n",
        "\n",
        "    elif lstm_a_cnn_la:\n",
        "\n",
        "        lstm_atten_cnn = create_lstm_atten_cnn(data_train_x=data_train_x, learn_rate=learn_rate, dropout_rate=0.4)\n",
        "\n",
        "        combinedInput = concatenate([lstm_atten_cnn.output, dense_num.output])\n",
        "\n",
        "\n",
        "    x = Dense(64, kernel_initializer=keras.initializers.glorot_uniform(seed=66), activation = 'relu')(combinedInput)\n",
        "\n",
        "    x = Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed=66), activation=\"linear\")(x)\n",
        "\n",
        "    # define the strutcure of the input to the model \n",
        "    if dense_la:  \n",
        "        \n",
        "        model = Model(inputs=[dense.input, dense_num.input], outputs=x)\n",
        "\n",
        "    elif bi:\n",
        "        \n",
        "        model = Model(inputs=[bilstm.input, dense_num.input], outputs=x)\n",
        "\n",
        "    elif lstm_cnn_la:\n",
        "\n",
        "        model = Model(inputs=[lstm_cnn.input, dense_num.input], outputs=x)\n",
        "\n",
        "\n",
        "    elif lstm_a_cnn_la:\n",
        "\n",
        "        model = Model(inputs=[lstm_atten_cnn.input, dense_num.input], outputs=x)\n",
        "\n",
        "    adam = keras.optimizers.Adam(lr=1e-3, decay=1e-6)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learn_rate)\n",
        "\n",
        "    model.compile(loss=\"mse\", optimizer=\"adam\" , metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX3hSoja4qlG"
      },
      "outputs": [],
      "source": [
        "def summarize_results(grid_result, model_name, result_to_file = True):\n",
        "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "    means = grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "    for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "    if result_to_file == True:\n",
        "        string_to_save = f\"Best: {grid_result.best_score_} using {grid_result.best_params_} \\n\" \n",
        "        for mean, stdev, param in zip(means, stds, params):\n",
        "            string_to_save = string_to_save + f\"{mean} ({stdev}) with: {param} \\n\" \n",
        "        text_results = open(model_name + \".txt\", \"w\")\n",
        "        n = text_results.write(string_to_save)\n",
        "        text_results.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rwZlEVKwxCN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dulxd0Oi0eoB"
      },
      "outputs": [],
      "source": [
        "# each time the training set is the 4/5 of the normal data or a fixed number \n",
        "train_test_split = int(round(len(data_train_x)/5, 0)*4)\n",
        "\n",
        "#train_test_split = 4000 \n",
        "train_test_split "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XNbPzuSRwynm"
      },
      "outputs": [],
      "source": [
        "train_different_manner = True\n",
        "if train_different_manner:\n",
        "\n",
        "    learn_rate = [0.0001, 0.00001]\n",
        "\n",
        "    epochs= [10, 13, 15]\n",
        "\n",
        "    #dropout_rate =[0.3, 0.4, 0.5]\n",
        "\n",
        "    for i in range(len(learn_rate)):\n",
        "\n",
        "        for j in range(len(epochs)):\n",
        "\n",
        "            #if for dense move the code under the dropout_rate to the right\n",
        "            #for z in range(len(dropout_rate)):\n",
        "\n",
        "\n",
        "            final_model = concatenated_dense(data_train_x[:train_test_split], train_x_tensor_pronouns[:train_test_split], learn_rate=learn_rate[i], dense_la=False, bi=True)\n",
        "            final_model.fit([data_train_x[:train_test_split], train_x_tensor_pronouns[:train_test_split]], train_y_tensor[:train_test_split],  epochs = epochs[j])\n",
        "\n",
        "            predicted2 = final_model.predict([data_train_x[train_test_split:], train_x_tensor_pronouns[train_test_split:]])\n",
        "\n",
        "            \n",
        "            predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "\n",
        "\n",
        "            # if dense_la True uncomment it. \n",
        "            #if(i==0 and j==0 and z==0):\n",
        "\n",
        "            #comment it out if dense_la true \n",
        "            if(i==0 and j==0):\n",
        "\n",
        "                model_text = \"\" + \"\\n\" + \"learn_rate: \" + str(learn_rate[i]) + \" epochs \" + str(epochs[j]) + \"\\n\" # + \"dropout_rate\" + dropout_rate[z]\n",
        "            else:\n",
        "                model_text = model_text + \"\\n\" + \"learn_rate: \" + str(learn_rate[i]) + \" epochs \" + str(epochs[j]) + \"\\n\"\n",
        "\n",
        "            #accuracy\n",
        "            model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "            print(\"Accuracy:\", round(accuracy_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "\n",
        "\n",
        "            #precision\n",
        "            model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "            print(\"Precision:\", round(precision_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "            #recall\n",
        "            model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "            print(\"Recall:\", round(recall_score(train_y_tensor[train_test_split:], predicted2), 5))\n",
        "\n",
        "            #F1Score\n",
        "            model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "            print(\"F1score:\", round(f1_score(train_y_tensor[train_test_split:], predicted2), 5)) \n",
        "\n",
        "\n",
        "    print(model_text)\n",
        "\n",
        "    name = \"Noise_Results_BERT_BiLSTM_reddit_500_pronouns_notcross.txt\"\n",
        "\n",
        "    with open(name, \"w\") as text_file:\n",
        "            text_file.write(model_text)\n",
        "\n",
        "    !gsutil cp /content/{name} gs://masterthesisbert/parameter_optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with the whole dataset \n",
        "final = concatenated_dense(data_train_x, train_x_tensor_diseases, 0.0001, 0.4, dense_la=False, bi=False, lstm_a_cnn_la=True)\n",
        "final.fit(x = [data_train_x, train_x_tensor_diseases], y =  train_y_tensor, epochs=10)"
      ],
      "metadata": {
        "id": "8DmZouN-V3ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY47jlXiHrAl"
      },
      "outputs": [],
      "source": [
        "validation = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiioguqlJpl7"
      },
      "outputs": [],
      "source": [
        "if validation:\n",
        "\n",
        "    predicted2 = final.predict([data_train_x2,train_x_tensor2_diseases])\n",
        "\n",
        "    predicted2 = np.where(predicted2<0.5, 0, 1)\n",
        "    #predicted2 = np.where(predicted2==np.max(predicted2, axis=1), 1, 0)\n",
        "\n",
        "    model_text = \"\"\n",
        "\n",
        "    model_text = model_text + \"BERT LSTM-A-CNN diseases\"\n",
        "    print(model_text)\n",
        "    #accuracy\n",
        "\n",
        "    model_text = model_text + \"\\n\" + \"Accuracy:\" + str(round(accuracy_score(train_y_tensor2, predicted2), 5)) \n",
        "    \n",
        "    print(\"Accuracy:\", round(accuracy_score(train_y_tensor2, predicted2), 5))\n",
        "\n",
        "\n",
        "    #precision\n",
        "    model_text = model_text + \"\\n\" + \"Precision:\" + str(round(precision_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "    print(\"Precision:\", round(precision_score(train_y_tensor2, predicted2), 5))\n",
        "    #recall\n",
        "    model_text = model_text + \"\\n\" + \"Recall:\" + str(round(recall_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "    print(\"Recall:\", round(recall_score(train_y_tensor2, predicted2), 5))\n",
        "\n",
        "    #F1Score\n",
        "    model_text = model_text + \"\\n\" + \"F1score:\" + str(round(f1_score(train_y_tensor2, predicted2), 5)) \n",
        "\n",
        "    print(\"F1score:\", round(f1_score(train_y_tensor2, predicted2), 5))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Second_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}